\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter*[Preface]{Preface}
\addcontentsline{toc}{chapter}{Preface}

In order to provide the reader with a reasonably clear understanding of the complete journey from the reactors and detectors to Daya Bay's raw data and then to our final results, we discuss each step in sequence, aiming to give at least a minimal amount of detail. This author has contributed to a number of these steps, but given the complexity of the experiment and its analysis, our results are necessarily built upon the work of the many others who have contributed to Daya Bay over the years. This thesis focuses primarily on our high-level analysis work, as detailed mainly in \Autoref{chap:selection,chap:bkg,chap:accDMC,chap:cutVary}. Discussions of hardware and lower-level analysis are provided for completeness, but kept relatively concise, with additional details provided for some of the areas in which this author contributed personally. Certain elements of the high-level analysis, including the reactor model, the fitting framework, and some of the subdominant background predictions, are adopted from the work of others, to which citations are provided. For the interested reader, we review these subjects in the Appendix.

% Collaboration is an essential feature of science, and this is certainty true when it comes to Daya Bay. The results in this thesis would have been impossible without the efforts of many individuals who have contributed to the experiment (and its analysis) over the years. In order to present a coherent whole, this thesis attempts to adequately detail every relevant feature of the experiment, and every step along the journey from raw data to final result. Some of those details represent this author's original work, while others are the work of fellow collaborators. In the text, we have aimed to be clear about which contributions are our own, and for those that aren't, we provide references. For the sake of any reader who wishes to dive into this author's original work, we highlight here some of the author's personal contributions to the experiment and analysis, with references to further discussion in the text.

Working on Daya Bay has given this author the opportunity to contribute to the experiment in a range of areas. In order to provide some context, we list a number of our main contributions, along with references to corresponding sections of the text.\footnote{The order of this list roughly follows that of the text.}

\begin{itemize}
  \item Onsite involvement in PMT testing and installation, commissioning of EH2-AD2 and EH3-AD4, and data-taking (\autoref{chap:experim}).
  \item Implementation of the algorithm for calculating the calibrated time of each PMT hit, and preparation of the calibration constants (\autoref{sec:calibTiming}).
  \item Involvement in development and implementation of the algorithm for calculating the amount of calibrated charge for each PMT hit, with emphasis on the response of the electronics to multiple hits spaced closely in time (\autoref{sec:calibHitCharge}).
  \item Involvement in the generation of calibration constants for the energy reconstruction (\autoref{sec:reconEnergyScale}) and the automation of this system.
  \item Implementation and validation (but not the original development) of the time-dependent nonuniformity correction for the energy reconstruction (\autoref{sec:reconEnergyNU}).
  \item Contributions to and maintenance of various components of \texttt{NuWa}, Daya Bay's offline analysis framework (\autoref{sec:selProd}), particularly the components related to time, charge, and energy calibration and reconstruction.
  \item Involvement in the migration of Daya Bay's data production process from NERSC's legacy PDSF computing cluster to the Cori high-performance computing system, requiring a bottom-up redesign of the production system. The $\sim$500~TB of data in the P17B dataset was successfully processed on Cori, with an order-of-magnitude improvement in production time.
  \item A modular and extensible C++ framework for high-performance processing of event streams in ROOT format, allowing for filtering events, transforming them, collecting statistics, and searching for time-correlated event clusters, such as IBD double coincidences \cite{SelectorFramework}. Arbitrarily complex event selections can be rapidly implemented on top of this framework. A suite of scripts and tools enable deployment on Slurm-compatible high-performance computing systems for processing petabyte-scale datasets.
  \item An IBD (and singles) selection \cite{IbdSel} built on the aforementioned framework (\autoref{chap:selection}). The performance and flexibility of this implementation makes possible the rapid and systematic exploration of changes to the IBD selection criteria (\autoref{chap:cutVary}).
  \item Cross-checks of the consistency of IBD candidate samples between different analysis groups, prior to publication of official oscillation results.
  \item Co-convenorship of the Daya Bay Data Quality Working Group, sharing responsibility for data quality in Daya Bay publications since 2017 (\autoref{sec:selDataQuality}). Development of various data quality procedures, tools, and documentation. Maintenance and repair of data quality database. Investigation of anomalous data.
  \item Development of a website \cite{dybvdq} for data quality activities, enabling exploration of data quality via various metrics, interactive (un)tagging of bad data, and reviews of the tagging decisions made by data quality shifters (\autoref{fig:selDybvdqShot}). This website made it much easier to carry out a data quality shift, increasing participation in this important role by members of the Collaboration.
  \item Original development of the LBNL $^9$Li analysis (\autoref{sec:bkgCosmo}), which was further extended by Marshall \cite{ChrisLi9} to support spectrum extraction and improved calculations of efficiencies and uncertainties. This work forms, together with other independent evaluations, the basis of the $^9$Li/$^8$He rates used in official Daya Bay results.
  \item A novel method for analytically calculating the singles rate (and, by extension, the accidentals rate and multiplicity cut efficiency) using the Lambert W function (\autoref{sec:singratescalc}).
  \item A Monte Carlo method for computing the statistical uncertainty of the accidentals rate (\autoref{sec:accStatUnc}).
  \item Various contributions to (but not the original development of) the LBNL toy Monte Carlo and oscillation fitter (\Autoref{chap:fitting,chap:fittingDetails}), including reorganization of the code, documentation of internals, updates for new datasets, integration with our IBD selection, and, most notably, significant performance improvements. Using aggressive multi-threading and process-based concurrency, runtime was reduced by an order of magnitude (\autoref{sec:cutVaryTechReq}), making possible our cut-variation study.
  \item Monte Carlo methods for predicting the muon veto efficiency from the rates of different classes of muons (\autoref{sec:cutVaryMuVetoEff}).
  \item Techniques to correct background rates for changes in IBD selection criteria, without rerunning the original background analyses (\autoref{chap:cutVary}).
  % \item Detailed studies of the differences between the IBD selections of different analysis groups.
  \item Development of methods for measuring the neutron-capture spectra and calculating the efficiency of the delayed-energy cut (\autoref{sec:cutVaryDelCutEff}).
  \item An implementation of vertex cuts for the nGd oscillation analysis (\autoref{sec:cutVaryVertexCut}), an analysis in which such cuts have previously been minimally investigated (in contrast to the nH analysis.)
  \item Ongoing use of the infrastructure described in this thesis, particularly the ability to study cut variations (\autoref{chap:cutVary}), in the preparation of upcoming Daya Bay oscillation-analysis results. These methods enable demonstration of the minimal cut-sensitivity of the results, and assignment of a systematic to account for any residual sensitivity.
  % \item Studies of nonuniformity in EH3-AD4 etc.
  % \item Technical implementation of AdSimpleNL.
    % \item Maintenance of CQ DB. Scripts, procedures, docs.
  % \item Remote DAQ for the tabletop NL measurement.
  % \item Muon track reconstruction?
  % \item Detective work, e.g. NominalCharge bug.
  % \item Calibration automation (spallation neutrons).
  % \item Integration of all the pieces of the chain, consistency of inputs etc.
  % \item DQ shift procedures.
\end{itemize}

A note on terminology: Throughout this work, the term ``spectrum'' will typically be used to refer to both the \emph{rate} and the \emph{spectral shape} (e.g., in terms of reconstructed energy) of some process. More concretely, a spectrum can be thought of as an energy histogram. When it is clear from the context, ``spectrum'' may occasionally refer to only the spectral shape; when we need to be more explicit, ``shape'' will be used in situations where the rate is irrelevant.

\end{document}