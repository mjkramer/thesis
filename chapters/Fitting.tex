\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Fitting}
\label{chap:fitting}

\section{Overview}
\label{sec:fitoverview}

In order to extract neutrino oscillation parameters, Daya Bay data is compared to the predictions associated with different parameter values, and the extracted parameters are then those that give the best fit to the data. Given knowledge of the reactor flux, detector response, and expected backgrounds, it is conceptually straightforward to generate a set of predictions. However, incorporating systematic and statistical uncertainties, and then assigning error bars to the extracted parameters, is where the procedure becomes more subtle and complex. In Daya Bay, separate analysis groups have historically employed two different approaches, theoretically equivalent but implemented very differently. These are the method of pull terms, and the covariance matrix approach. In this analysis, we use the latter, but both will be briefly described in what follows.

\subsection{Method of pull terms}
\label{sec:pullterms}

In the method of pull terms, the fitter is ``smart'' in the sense that it has knowledge of the underlying models (reactor, detector, background, etc.) and knows how their predictions will vary under different assumptions regarding systematic uncertainties. In this approach, each systematic is parameterized by a \emph{pull term} (or \emph{nuissance parameter}), which is in turn assigned an uncertainty of its own. An example of such a pull term might be the relative energy response of a given AD. Each pull term is assigned a nominal value, corresponding to our best estimate given available knowledge. Then, during the fit, not only are the oscillation parameters varied, but so are the pull terms, and the predictions are transformed accordingly. The total $\chi^2$ then takes a form similar to
\[ \chi^2 = \sum_i \frac{(x_i - \widebar x_i)^2}{\sigma_i^2} + \sum_j \frac{(\eta_j - \widebar \eta_j)^2}{\varsigma_j^2}, \] where $x_i$ are the measured data (e.g., AD spectra), $\widebar x_i$ are the predictions (which vary as we scan the oscillation parameters and pull terms), $\sigma_i$ are the \emph{statistical} uncertainties on the data, $\eta_j$ are the pull terms, $\widebar \eta_j$ are their nominal values, and $\varsigma_j$ are the uncertainties on the pulls.

Fitting is complete when the fitter has found the values of the oscillation parameters \emph{and pull terms} that minimize the total $\chi^2$. The 1$\sigma$ error bars on the oscillation parameters are then based on the amount of variation required to increase the reduced $\chi^2$ by one unit [XXX not one unit if fitting more than one parameter; see e.g. doc-8774 p29 and its ref 22] (minimizing over the pull terms at every step). Correlations between spectral energy bins are handled implicitly; the information is encoded in the manner in which different bins move together when pull terms are varied.

\subsection{Covariance matrix approach}
\label{sec:covmatapproach}

As an alternative to using pull terms, uncertainties and correlations can be encoded in a single covariance matrix generated using Monte Carlo techniques. In this approach, the fitter is relatively ``dumb'': It knows only how to generate a prediction using a \emph{nominal} model (of, again, reactors, backgrounds, detectors, etc.) and how to vary the prediction for different values of the oscillation parameters. It has no idea how the prediction will transform under varying assumptions with respect to systematic uncertainties. (This knowedge belongs to the Monte Carlo.) The fitter's job is simply to take the measurements $x_i$, the predictions $\widebar x_i$ (which vary according to the oscillation parameters), and the covariance matrix $V_{ij}$, and then to find the oscillation parameters which minimize the $\chi^2$,
\[ \chi^2 = (x_i - \widebar x_i) V_{ij}^{-1} (x_j - \widebar x_j). \]

In practice, the full NuWa-based Monte Carlo is not used for generating the covariance matrix, due to its complexity and computational cost. Instead, a ``toy'' MC, described in the next section, was developed for this purpose. Following the discussion of the toy MC, we describe the oscillation fit and uncertainty calculation in detail.

\section{Toy Monte Carlo}
\label{sec:toymc}

The toy MC essentially acts as a generator of ``fake'' experiments (or ``toys''), as represented by the prompt spectrum measured by each AD. Each toy may include fluctuations due to statistics and/or a chosen set of systematics. This basic functionality enables the production of three essential inputs used by the fitter:

\begin{itemize}
\item The covariance matrix used for calculating $\chi^2$ during the fit.
\item The ``super histograms'': The nominal (i.e. unfluctuated) predicted cross section-weighted antineutrino flux produced by each core. This is used for extrapolating near-site measurements to the far site.
\item The conversion matrix between prompt and ``true'' antineutrino energy, also used in the extrapolation.
\end{itemize}

In addition, the toy MC provides a method of validating the fitter, since toys can be generated for any chosen values of $\tAC$ and $\Dmsqee$, thus enabling the testing of the fitter's ability to recover the same values.

\begin{comment}
  The toy MC also generates a ``PredictedIBD'' file which contains the background-free no-oscillation IBD spectra of each detector. As far as I can tell, this is only used in order to calculate a `summed' covariance matrix in which the matrices of the three stages (6, 8, 7AD) are combined, with the weighting determined by the PredictedIBD counts. (Oscillations shouldn't affect this weighting between ADs in the same hall, or the weighting between different stages.) In turn, the summed matrix is not used during the fit, but is only produced as a diagnostic. 
\end{comment}

\end{document}
