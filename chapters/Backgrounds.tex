\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Background subtraction}
\label{chap:bkg}

While Daya Bay's design ensures a fairly pure sample of antineutrino events, a small contamination of backgrounds (at the percent level) is unavoidable. These backgrounds can be subdivided into \emph{correlated} and \emph{uncorrelated} backgrounds. The uncorrelated backgrounds consist entirely of accidental coincidences between singles events, and the rate and spectrum can be easily estimated from that of the singles. On the other hand, the correlated backgrounds are so named because the prompt and delayed pulses are correlated in time, as they both originate from a single underlying process. The correlated background in Daya Bay correspond to four distinct processes, and each one requires its own technique for determining the rate and spectrum.

In this chapter, we discuss the measurement of each of these backgrounds. Once they have been measured, their scaled prompt spectra can be subtracted from that of IBD candidates, allowing the oscillation fit to proceed with more pure prompt spectrum, albeit one with an additional uncertainty stemming from the imprecise nature of the background measurements.

Before considering these double-coincidence backgrounds, we begin by discussing a background defined at the level of \emph{individual} triggers, namely, the so-called ``flashing'' of PMTs. Reduction of these ``flashers'' is necessary in order to minimize the rate of the uncorrelated backgrounds.

\section{PMT light emission (``flashers'')}
\label{sec:bkgFlashers}

During detector commissioning, some PMTs were found to occasionally emit light due to arcing in their bases. At any given time, a dozen or two PMTs in each AD will have the tendency to flash brightly enough to trigger the detector. Some flashers can produce as much as 100~MeV of reconstructed energy. Within the delayed energy region of 6-12~MeV, the flasher rate has averaged at around 0.7~Hz for each AD. These ``delayed-like'' flashers, if included in the analysis, would significantly increase the rate of backgrounds caused by the accidental coincidence of two uncorrelated signals. As discussed in \autoref{sec:accbkg} and \autoref{chap:accDMC}, the rate of such ``accidentals'' is proportional to the rate of delayed-like signals, and this rate (excluding flashers) ranges from around 0.05~Hz at EH3 to 1~Hz at EH1. While this would merely (roughly) double the 1\% accidental background in the near halls, in the far hall it would increase this background by an order of magnitude to the 10\% level, counter to Daya Bay's goal of perecent-level background contamination.

Fortunately, flashers are easily distinguished from ``physical'' singles due to their unique pattern of light emission, enabling them to be removed from the analysis with high efficiency while minimally affecting true IBDs. This light pattern is characteristized by two ``hot spots'' on opposite sides of the AD. When a PMT base emits light, much of the light is absorbed by the black radial shield and conical magnetic shield. The remainder escapes within a conical profile; some of the photons will strike the flasher's photocathode (resulting in the flashing PMT having the highest charge), and others will primarily illuminate the PMTs across the AD from the flasher, especially the one that lies directly opposite to it. In addition, the time distribution of PMT hits is broadened for flashers due to the geometry of light propagation across the AD. By taking advantage of these telltale distributions of charges and times, it is possible to achieve excellent discrimination of flashers from physical events.

The flasher identification criteria were developed in a somewhat ad-hoc fashion, by defining quantities that could conceivably serve as discriminators, and then further defining combinations of these quantities, and finally plotting the distributions of these (combined) quantities until a clean separation between flashers and non-flashers was apparent. It is far more important to minimize (IBD) signal inefficiency instead of maximizing flasher rejection, because a small amount of unrejected flashers will simply slightly increase the rate of accidental backgrounds (which can be easily quantified), whereas a signal inefficiency could vary among the ADs and thereby bias the oscillation fit.

\newcommand\fmax{f_{\mathrm{max}}}
\newcommand\fquad{f_{\mathrm{quad}}}
\newcommand\fID{f_{\mathrm{ID}}}
\newcommand\fPSD{f_{\mathrm{PSD}}}

Early in the experiment, this prolonged and interative process eventually gave rise to the \emph{ellipse cut} (based on the charge distribution) and the \emph{PSD cut} (based on the time distribution), which demonstrated excellent performance, and these cuts continue to be used in this analysis. The ellipse cut is based on two quantities, termed $\fmax$ and $\fquad$. The first, $\fmax$, is simply the ratio of $Q_{\mathrm{max}}$ (the maximum individual PMT charge across all PMTs) over the total charge $Q_{\mathrm{tot}}$:
\begin{equation*}
  \fmax = \frac{Q_{\mathrm{max}}}{Q_{\mathrm{tot}}}.
\end{equation*}
For flasher events, $Q_{\mathrm{max}}$ belongs to the flashing PMT itself, and $\fmax$ is typically higher for flashers than for physics events. However, physical events near the edge of the detector can exhibit high $\fmax$, so this variable alone is insufficient to cleanly discriminate flashers. As such, we also consider $\fquad$, which is based on dividing the AD into four quadrants: ``Quadrant 1'' (q1) is the one that is centered on the highest-charge PMT; q3 is the one across from q1; and q2 and q4 are the two ``to the side.'' $\fquad$ than captures the conical nature of the light emission:
\begin{equation*}
  \fquad = \frac{Q_{\mathrm{q3}}}{Q_{\mathrm{q2}} + Q_{\mathrm{q4}}}.
\end{equation*}
Like $\fmax$, $\fquad$ alone is not a good discriminator, due to overlap between flashers and physics events. However, their combination
\begin{equation*}
  \fID = \log_{10} \left[ \fquad^2 + \left( \frac{\fmax}{0.45} \right)^2 \right]
\end{equation*}
turns out to be an excellent discriminator. Indeed, as shown by (XXX Fig. 21 of the long paper), requiring $\fID < 0$ reduces the flasher rate to a negligible level, and many analyses have relied on $\fID$ alone to identify flashing 8" PMTs. Still, even further flasher reduction can be achieved by incorporating timing information. To capture the broadening of the time distribution shown by flashers, we use the variable(s) $f_{\mathrm{t}1}$ ($f_{\mathrm{t}2}$), defined as the ratio of the number of hits in the first 200 (150)~ns of the signal, over the number of hits in the first 400~ns. The discriminator $\fPSD$ is then defined as
\begin{equation*}
  \fPSD = \log_{10} [4 \cdot (1 - f_{\mathrm{t}1})^2 + 1.8 \cdot (1 - f_{\mathrm{t}2})^2].
\end{equation*}
By requiring both $\fID < 0$ and $\fPSD < 0$, we eliminate virtually all 8" PMT flashers from the analysis. However, in addition to the 192 8" PMTs, there are six 2" PMTs located at the top and bottom of each AD along the calibration axes, and these can also flash. Such events were easily identified as those in which a 2" PMT saw an extreme amount of charge, with a cut of 100 PE providing essentially perfect separation between 2" PMT flashers and other events.

The exact efficiency of these cuts, in terms of identifying flashers, is unimportant, as long as it is high enough. Any residual flashers will automatically be counted in the singles rate, and thus so will their contribution to the accidental background rate.\footnote{There is a small second-order correction due to the fact that an accidental cannot be formed by two flashers in the same PMT, given that it takes on the order of a second for a PMT to ``recharge'' after flashing. However, this correction is negigible given the extremely low rate of residual flashers.} On the other hand, it \emph{is} important to study the signal inefficiency, i.e. the probability of improperly rejecting an IBD prompt or delayed trigger. If this inefficiency differs significantly among the ADs, it could bias the oscillation result. The inefficiency was estimated by taking a sample of IBD-like events (without the flasher cut) (XXX Xin's flasher slides), and making a 2D histogram of $(f_{\mathrm{prompt}},f_{\mathrm{delayed}})$, where $f$ is $\fID$ or $\fPSD$. The IBD-like sample consists of a small number of accidentals that contain a flasher (or two), and a much larger number of flasher-free pairs (including true IBDs, accidentals, and correlated backgrounds). The use of IBD-like events has the effect of diluting the presence of flashers in the sample, since a coincident pair is much more likely to come from an IBD than an accidental (which furthermore is more likely to have two physical singles rather than a flasher). The inefficiency was then determined by the degree to which the flasher-free ``blob'' extended into the regions rejected by the discriminator. These studies found that $\fID$ and $\fPSD$ each introduce an inefficiency of 0.02\%, with uncertainties of 0.01\% correlated and 0.01\% uncorrelated. Since we use both of them in this analysis, our estimated inefficiency is 0.04\%, with an uncertainty of 0.02\% correlated and 0.02\% uncorrelated.

XXX show 2D plot of ellipse cut from doc-7143.

\section{Accidental coincidences}
\label{sec:accbkg}

IBD-like pairs can be formed when two uncorrelated triggers (\emph{singles}) ``accidentally'' occur closely together in time. Given that the singles rate is a few dozen Hz in each AD, the characteristics of singles (and thus of accidentals) can be measured with extremely high statistical precision. Once the rates of prompt-like and delayed-like singles have been determined, the accidentals rate can be calculated via a straightforward application of Poisson statistics. This procedure is detailed in \autoref{sec:accratecalc}. The spectrum, meanwhile, is equal to that of the singles sample, whose extraction is described in \autoref{sec:selSingles}.

\begin{comment}
As previously stated, the accidental background can be straightforwardly measured based on the characteristics of singles events. The singles spectrum is first measured by searching for prompt-like events that satisfy the usual muon vetos but are separated from other prompt-like events by at least 400~$\mu$s. The total integral of this spectrum gives the prompt-like rate $R_p$, while the integral above 6~MeV gives the delayed-like rate $R_d$. The accidental background rate is then simply
\[ R_\mathrm{acc} = R_d(1 - e^{-R_p\Delta t})e^{-2R_p\Delta t}, \] where the factor in parentheses is the probability for a prompt-like single to fall within the $\Delta t$~=~200~$\mu$s preceding a delayed-like single, and the final factor is the probability that the event is \emph{not} rejected by the decoupled multiplicity cut, which disallows any additional prompt-like single in the 400~$\mu$s preceding the delayed event. Once the rate has been determined this way, the spectrum is trivial: It is simply the singles spectrum itself.
\end{comment}

\begin{comment}
  Mention IHEP's cross-check, and the additional uncertainty stemming from the difference between it and the nominal result?
\end{comment}

\section{Cosmogenic $^9$Li/$^8$He}
\label{sec:bkgCosmo}

\newcommand\linine{$^9$Li}

The dominant correlated background for Daya Bay comes from the isotopes $^9$Li and $^8$He, which are produced as spallation products of carbon when energetic muons traverse the AD. These two isotopes have relatively long livetimes of 257~ms for $^9$Li and 172~ms for $^8$He; thus, while the majority of them will decay within the O(1~s) veto window that follows showering muons, a non-negligible fraction will survive past it. When one of these isotopes undergoes beta decay into particular excited states of the daughter nucleus, the daughter will immediately emit a neutron (and, in the case of $^9$Li, further disintegrate into two alphas). The relevant beta decay endpoints extend up to 12~MeV, placing these decays squarely within the IBD prompt energy cut. The combination of this beta decay and the subsequent nGd capture produces the characteristic double-coincidence signature of an IBD event. In what follows, we will collectively refer to these two isotopes as $^9$Li, given that it is believed to be the predominant of the two. (For instance, the KamLAND collaboration's FLUKA simulations \cite{KamLAND_cosmo} indicated a 10:1 ratio of $^9$Li to $^8$He production.) Later we will propagate the uncertainty on this ratio into the background estimation. 

Although at Daya Bay there is no feasible way of distinguishing \linine\ decays from IBDs at the level of individual events, it is still possible to statistically estimate the total \linine\ rate by exploiting the correlation in time between \linine\ events and preceding muons. To see this, let the muon rate be denoted by $R_\mu$, and the \linine\ livetime by $\tau$. Since IBD events are uncorrelated with muons, the time between an IBD and the most recent preceding muon is given, according to Poisson statistics, by the PDF
\begin{equation}
  \label{eq:cosmoBkgPdfIbd}
  f_{\mathrm{IBD}}(t) = R_\mu e^{-R_\mu t}.
\end{equation}
Meanwhile, a \linine\ decay is correlated in time with the parent muon:
\begin{equation}
  \label{eq:cosmoBkgPdfLiProd}
  f_{\mathrm{Li,\,Parent}}(t) = \frac{1}{\tau} e^{-t/\tau}.
\end{equation}
However, in the time between a muon shower and an associated \linine\ decay, additional muons may be detected, and these may closely resemble the true parent muon. As such, the quantity that we can reliably measure is not the time between the \linine\ decay and the parent muon, but (as with IBDs) the time between the decay and the \emph{last} muon. This detail has the effect of modifying the time constant in \autoref{eq:cosmoBkgPdfLiProd}. To derive this modification, let us consider a \linine\ event with a time-to-last-muon of $t$. Either the last muon was the parent muon, or it wasn't. Defining our time axis with the decay at the origin, these two possibilities can be stated quantitatively as:
\begin{enumerate}
\item The parent muon occured at time $-t$, with no intervening uncorrelated muons, or
\item The parent muon occured prior to $-t$, and the most recent uncorrelated muon occured at $-t$.
\end{enumerate}
The parent muon's time PDF is given by \autoref{eq:cosmoBkgPdfLiProd}, and that of the most recent uncorrelated muon by \autoref{eq:cosmoBkgPdfIbd}. Meanwhile, the Poisson probability of observing zero uncorrelated muons in a time window of $t$ is given by $e^{-R_\mu t}$. Thus, letting $P_1$ and $P_2$ be the probabilities\footnote{Here we will say ``probability'' when we really mean ``probability density''.} of the two cases above, we have
\begin{align*}
  P_1 = \frac{1}{\tau} e^{-t/\tau} \cdot e^{-R_\mu t} = \frac{1}{\tau}e^{-(R_\mu + 1/\tau)t}
\end{align*}
and
\begin{align*}
  P_2 = \int_t^\infty \frac{1}{\tau}e^{-t'/\tau}\,dt' \cdot R_\mu e^{-R_\mu t} = e^{-t/\tau} \cdot R_\mu e^{-R_\mu t} = R_\mu e^{-(R_\mu + 1/\tau)t}.
\end{align*}
Letting
\begin{equation*}
  \lambda = R_\mu + 1/\tau,
\end{equation*}
we finally add $P_1$ and $P_2$ to obtain the PDF of time-to-last-muon for \linine\ decays:
\begin{equation}
  \label{eq:cosmoBkgPdfLi}
  f_{\mathrm{Li}}(t) = \lambda e^{-\lambda t}.
\end{equation}

Comparing \autoref{eq:cosmoBkgPdfLi} and \autoref{eq:cosmoBkgPdfIbd}, we see that, if $\lambda$ is sufficiently different from $R_\mu$ (in other words, if $R_\mu$ is sufficiently small), then there will be a measurable difference between the time-to-last-muon PDFs of \linine\ and IBD events. In this case, the separate event counts can be obtained by constructing a histogram of time-to-last-muon from a mixed sample of IBDs and \linine\ events. This histogram can then be fit to a weighted sum of \autoref{eq:cosmoBkgPdfLi} and \autoref{eq:cosmoBkgPdfIbd}, with the weights (as detemined by the fit) corresponding to the number of events in the two categories:
\begin{equation*}
  f(t) = N_{\mathrm{Bkg}} \lambda e^{-\lambda t} + N_{\mathrm{IBD}} R_\mu e^{-R_\mu t}.
\end{equation*}

If the fit is allowed to extend down to $t$ of a couple dozen ms, an additional correlated component must be considered resulting from accidental double coincidences of $^{12}B$ and/or $^{12}N$ decays. These two isotopes, with respective lifetimes of 29 and 16~ms, and $\beta^\pm$ endpoints of 13.7 and 16.3~MeV, can be produced multiple times by a single muon shower, and their decays extend well into the delayed energy region. Their double coincidences are already considered as part of the accidental background estimation\footnote{There is some bias owing to the fact that the prompt and delayed $^{12}$B/$^{12}$N are not uncorrelated in time (as assumed in the accidentals calculation), but are in fact correlated by virtue of their usually coming from the same parent muon. However, the rate of such events is extremely low, as can be seen in the fits shown later in this chapter, so there is no need to attempt a correction to the accidentals rate.}, so we are not concerned with measuring them here as a separate background; however, they do distort the time-to-last-muon histogram at low times, and thus they must be considered in order to extract the \linine\ component accurately. Past fits (XXX cite) to the spectrum of muon-correlated singles have indicated that $^{12}$B is by far the dominant of these isotopes, so in what follows we treat it as the only one of relevance.

When fitting the time-to-last-muon for $^{12}$B double coincidences (BB events, for short), the time constant must be considered carefully. We first consider the time to the \emph{parent} muon, rather than to the last muon. For single $^{12}$B events, the corresponding time constant is simply the $^{12}$B lifetime $\tau_{\mathrm{B}}$.  However, if a muon produces two $^{12}$B nuclei, then a timespan of $\tau_{\mathrm{B}}$ will correspond to a $1/e$ survival probability of one nucleus, and likewise for the other. The observation of one decay, of either nucleus, constitutues an observation of the whole pair, since we are recording the time between muons and the \emph{prompt} event in an IBD candidate. The probability of nonobservation of the pair, then, is the probability of seeing \emph{neither} nucleus decay, which is $1/e^2$ for a timespan of $\tau_{\mathrm{B}}$. That is, the time constant for the BB time-to-parent-muon distribution is $\tau_{\mathrm{B}}/2$. For the time to the \emph{last} muon, rather than the parent, the arguments preceding \autoref{eq:cosmoBkgPdfLi} imply a rate constant of $\lambda_{\mathrm{BB}} = R_\mu + 2/\tau_{\mathrm{B}}$.
\begin{comment}
\footnote{We neglect the possibility of only seeing \emph{one} of the nuclei decay, since the IBD coincidence cut of \us{200} is far shorter than the $^{12}$B lifetime of 29~ms, and therefore, the observation of one decay effectively \emph{implies} the observation of the other.}

 As such, the time constant for BB events is $\tau_{\mathrm{B}}/2$. (I'm not sure I buy this. We are \emph{assuming} that the two decays occur within 200~us of each other, and the probability of seeing them both is basically equivalent to the probability of seeing the first, and the time-to-last-muon PDF for the first just has the time constant of $\tau$. Where the time constant \emph{does} get halved is if we consider a muon that produces two nuclei and we want the probability of seeing none i.e. the time to the next decay.)
\end{comment}

\begin{comment}
  XXX our code has the 12B half-life (20 ms) listed as the lifetime (should be 29 ms). This further exacerbates the use of the mistaken(???) factor of 2??? In total our time constant has been off by a factor of 3 (well, plus the Rmu part)
\end{comment}

To finish deriving the full expression used in fitting the time to last muon, we simply add a factor $r$ corresponding to the $^8$He fraction, giving
\begin{equation}
  f(t) = N_{\mathrm{Bkg}} \left( (1-r)\lambda_{\mathrm{Li}} e^{-\lambda_{\mathrm{Li}} t} + r\lambda_{\mathrm{He}} e^{-\lambda_{\mathrm{He}} t }\right) + N_{\mathrm{BB}} \lambda_{\mathrm{BB}} e^{-\lambda_{\mathrm{BB}} t} + N_{\mathrm{IBD}} R_\mu e^{-R_\mu t},
\end{equation}
where
\begin{align*}
  \lambda_{\mathrm{Li}} &= R_\mu + 1/\tau_{\mathrm{Li}} \\
  \lambda_{\mathrm{He}} &= R_\mu + 1/\tau_{\mathrm{He}} \\
  \lambda_{\mathrm{BB}} &= R_\mu + 2/\tau_{\mathrm{B}}.
\end{align*}

Although this method is simple in theory, a significant challenge arises from the fact that a minimum muon energy must be defined when calculating the time between each event and its most recent preceding muon. If this cut is too low, then the time between muons will be comparable to the \linine\ livetime, and with finite statistics, it will be difficult to reliably distringuish between the two components in the fit. Conversely, if the muon cut is too high, then some fraction of \linine-producing muons will be discarded, and those \linine\ events will not appear to be muon-correlated, leading to an underestimate of the rate.

This issue is mitigated somewhat by the fact that low-energy muons produce a relatively small portion of the total \linine\ rate.\footnote{Based on the results shown in this section, some 90\% (95\%) of the \linine\ at the near (far) site comes from muons above the 3e5-pe ``shower muon'' threshold.} However, an accurate assessment must still attempt to quantify this contribution. To enhance the \linine\ signal in the fit, either the muon sample or the \linine\ candidate sample (or both) must be somewhat purified. We use both methods in what follows.

For the muon sample, the goal is to remove muons that are unlikely to produce a \linine\ event. This will reduce the muon rate $R_\mu$, thus increasing the difference between the time constants of the fit components. However, the cost of this \emph{muon reduction} is that some fraction of \linine-producing muons will be discarded. The associated \linine\ events will have a time-to-last-muon rate constant of $R_\mu$ rather than $\lambda$, so they will not contribute to the measured \linine\ rate, leading to an inefficiency (and corresponding uncertainty) in the total rate estimate. Here, muon reduction is achieved by using \emph{neutron tagging}, in which we only consider those muons for which a neutron capture is observed in the immediate aftermath. Given the sizable uncertainty on the efficiency of the tagging, this method is only used where absolutely necessary, i.e., in the lowest bin of muon energy. The details are discussed later.

As for increasing the purity of the \linine\ sample, the method used here is simply to apply a cut on the prompt energy. As shown in Fig.~XXX, the \linine\ and IBD spectra differ substantially, with the former being much harder. As such, a higher prompt energy cut will increase the ratio of \linine\ to IBD events. This benefit, however, must be weighed against the loss in the total statistics arising from an aggressive prompt energy cut. Here, heuristically chosen cuts are used in order to obtain acceptable fits. As will be detailed later, more aggressive cuts are used in the near hall (where the IBD ``background'' fraction is larger) and for lower muon energies (where a greater \linine\ purity is needed due to the proximity of $\lambda$ to $R_\mu$). These prompt cuts, together with neutron tagging (for low muon energies), enable the \linine\ fit to be performed for muon energies that extend all the way down to the ``AD muon'' threshold of 3,000~pe.

In the sections that follow, we describe the \linine\ rate estimation in detail, beginning with the selection of \linine\ candidates, followed by the muon selection, the fitting procedure, its results, and their uncertainties.

\subsection{$^9$Li candidate selection}
\label{sec:bkgLi9Sel}

The $^9$Li selection is largely identical to the standard IBD selection, as described in \autoref{chap:selection}, with two key differences: First, the prompt energy cut is higher, as described above (although this cut is applied after the initial selection, for the sake of flexibility), and, secondly, the ``shower muon'' veto is disabled. That is, rather than vetoing for 0.4004~s after a muon of $>$3e5~pe, only the ``AD muon'' veto of 1.4~ms is applied. This is necessary because the shower veto would remove approximately 80\% ($1 - exp(-400/257)$) of the $^9$Li events produced by shower muons, greatly reducing the already-limited statistics of the sample. When producing the final \linine\ rate estimation, we must correct for the efficiency of this modified muon veto. Using the muon toy MC described in (XXX cut optimization chapter), these efficiencies were determined to be 87\%, 90\%, and 99\% in EH1, EH2, and EH3, respectively. (Compare to 82\%, 85\%, and 98\% for the standard muon veto.)

XXX the Li9 rates, as I've been using them so far, are calculated assuming the efficiencies of the standard muon veto, rather than the modified one. I've corrected the constants in Li9Calc accordingly, but need to rerun the calculation and update the plots etc.

\subsection{Muon selection}
\label{sec:bkgLi9MuonSel}

For the purposes of applying the muon veto and constructing the time-to-last-muon histograms, we use the muon tree generated by the Daya Bay software framework (\texttt{NuWa}) during data production. In this tree, muons that trigger multiple detectors are merged into a single muon, and prompt retriggers are discarded. This represents a minor difference from the IBD selection, where retriggers are \emph{not} discarded, leading to a slight reduction in the muon veto efficiency (which is nevertheless correctly accounted for in the summation of veto windows, and thus there is no resulting bias). The discarding of retriggers, in particular, helps to prevent the arising of a bias in the time constant for $^9$Li events. The precise details of the \texttt{NuWa} muon selection are as follows:

\begin{itemize}
\item A trigger in the inner (outer) water pool, with at least 6 (8) fired PMTs, is tagged as a WP muon.
\item A trigger in an AD of more than 3000 photoelectrons is tagged as an AD muon
\item Tagged muons that occur within a 300~ns window are merged into a single muon (with the WP hit counts and AD charges individually stored), with the timestamp taken to be the time of the earliest trigger.
\item Any subsequent ``muons'' that occur within 10~\us\ are deemed to be retriggers and are thus discarded
\end{itemize}

During the $^9$Li selection, a given muon object will result in the 1.4~ms AD muon veto if the object contains an AD muon for the detector in question. If not, but if there is a IWP or OWP muon present with \emph{more than 12 hit PMTs}, then the 600~\us\ WP muon veto is applied instead.

If a $^9$Li candidate is found, then all preceding muons (within a 10~second window) are stored in an array specific to that candidate. This array can then be looped over when constructing the time-to-last-muon histogram, skipping over muons outside of a chosen energy range until the most recent one (within the energy range) is found.

\subsubsection{Neutron tagging}
\label{sec:bkgLi9NeuTag}

In order to enable the neutron tagging technique (as needed for calculating the rate of $^9$Li produced by low-energy muons), an additional flag is stored for each muon, indicating whether the muon occurred in coincidence with a neutron capture candidate. Specifically, the neutron tag is applied if any trigger ranging from 1.8 to 12~MeV (XXX why is it 18~MeV in the code?) occurs between 20 and 200~\us\ after the muon. Although these cuts are heuristic, they are reasonable in principle: The energy range is wide enough to include captures on both gadolinium and hydrogen, and the time window includes most of the Gd captures and a decent fraction of H captures, while avoiding spurious signals that can arise from retriggers occurring in the first dozen or so \us\ after a muon.

The primary challenge in using the neutron tagging technique is accounting for the efficiency of the tagging. That is, what percentage of $^9$Li-producing AD muons (within a specified energy range) will be tagged? For sufficiently high-energy muons (roughly above 1.6e5 photoelectrons), the $^9$Li rate can be measured both with and without neutron tagging, and thus the efficiency can be extracted directly. However, for muons below 1.6e5~pe (where neutron tagging is \emph{required}), this is not possible, and the efficiency must be extrapolated. In XXX doc-10920, the efficiency is measured across a range of muon energies, and found to range from $\sim$90\% for muons above 1.8~GeV ($\sim$2.9e5~p.e.), down to $\sim$60\% for muons of 1.0--2.5~GeV (1.6--4e5~p.e.). For the sake of analysis, we thus assume a neutron tagging efficiency of unity (XXX should change this to 80\% in Li9Calc!!!), and apply a relative uncertainty of 45\% to the $^9$Li rate for muons below 1.6e5~p.e.

\subsection{Time-to-last-muon fit}
\label{sec:bkgLi9HistoFit}



- Forming and fitting the histogram
- - Detail the cuts we use for different muon energy ranges (prompt energy, neutron tag)
- Prompt cut efficiency
- - Extraction of spectrum. Subtraction of IBD spectrum.
- - Uncertainty
- - - Statistical - binomial confidence interval accounting for error bars on subtracted spectrum => 1-2\% (See if can find how Chris did it in code... sounds tricky)
- - - Systematic - Variations in time binning, muon PE cut in background sample => 1\%
- Neutron tag efficiency
- - Uncertainty - 45\% according to doc-10920
- - Nominal value of... 80\%? 60\%?
- Uncertainty from fit
- - Neutron tagging cutoff - 1.5e5 to 1.8e5 => 10\% (NOT IN CHRIS'S TABLE)
- - Binning => <5\% (NOT IN CHRIS'S TABLE) (MENTIONED AT END)
- - B12 => 8\%
- - He8 => 4\%
- Uncertainty of shower veto correction (He8 fraction)
- - Vary He8 fraction from 0 to 15\%
- Conversion from fit result to daily rate
- - Efficiencies of Li9 selection (ntag, pcut)
- - IBD selection efficiencies (veto, mult)
- - Shower veto correction
- - Propagation of uncertainties

\subsection{Spectrum}
\label{sec:bkgLi9Spectrum}

The \LiHe\ spectrum can be either extracted from data or predicted from nuclear tables. The two approaches give consistent results, and we briefly describe them both here, although our analysis makes exclusive use of the predicted spectrum.

An extraction of the spectrum from Daya Bay data was performed by Marshall in [XXX]. The approach takes advantage of the fact that \LiHe\ are essentially the only IBD-like events that are correlated with muons on the 100~ms timescale. A \LiHe-enriched sample was obtained by taking IBD-like events within 2---200~ms of a ``shower'' muon, here defined as one producing at least $2\times10^5$ photoelectrons. This sample contained various muon-uncorrelated ``backgrounds'', such as true IBDs and accidentals. In order to remove this contamination, a \LiHe-depleted sample was obtained by looking for IBD candidates with no preceding shower muons within 1.5~s. Before subtracting the two spectra, an appropriate normalization for the depleted sample had to be determined. This was done by performing the time-to-last-muon fit for the enriched sample, which indicated the number of true \LiHe\ events in the sample, in turn implying the number of non-\LiHe\ events. The depleted sample was thus normalized to this latter count, and the subtraction was performed, giving the results shown in Fig.~XXX.

The prediction of the \LiHe\ spectrum was carried out by Ochoa in [XXX docs 8772, 8860]. Three types of reference tables were consulted: nuclear structure, branching ratios, and measured spectra (of neutrons, alphas, and gammas). Given the number of decay pathways involved, a purely analytic approach was infeasible, so a toy Monte Carlo was used to produce random decay events. The discussion here uses the example of $^9$Li, but $^8$He was essentially treated the same way. The initial $\beta$ decay (into any of four $^9Be^*$ states) was simulated using the Fermi theory and the published energy levels and branching fractions. For the decays that produce a $2\alpha+n$ final state (i.e. the only decays of interest to us), $^9Be^*$ disintegrates via two consecutive two-body decays (via either $^8$Be or $^5$He). Since there is no angular distribution to consider in a two-body decay, the disintegration was treated using basic kinematics, with the width of each state modeled with a Breit-Wigner function. The result was a collection of simulated events, each one recording the (true) energies of the electron, the neutron, and the two alphas (or, in the case of $^8$He, the electron, the neutron, and the gamma).

In order to benchmark this simulation, its output was compared against published measurements of \LiHe\ spectra of neutrons, alphas, and gammas. Based on this comparison, one (particularly broad) level of $^8$He had to be augmented with a Gaussian density of states (a complication not considered for the other levels). Since the published branching ratios were relatively imprecise, they were hand-tuned (within limits) in the simulation so as to achieve satisfactory agreement with the published spectra.

Finally, to obtain the predicted spectrum in terms of prompt (i.e. reconstructed energy), the simulated events were passed through a model of the detector nonlinearity. Given that the \LiHe\ spectrum prediction was performed in 2013, it is based on an older nonlinearity model than the one discussed in \autoref{sec:fitToyDetResponse}.\footnote{The differences between models are not significant enough to warrant concern, especially in light of the uncertainty we assign to the \LiHe\ spectrum, as discussed in XXX ref toy MC.} Crucially, this model, produced by the BCW analysis group,\footnote{Brookhaven, Caltech, and William \& Mary.} includes nonlinearity curves for alphas and neutrons, whose kinetic energies were also considered when determining the prompt energy for each event. The resulting prompt energy spectra could then be combined according to the best estimate of the relative proportions of $^9$Li and $^8$He. For the sake of this analysis, a nominal 5.5\% fraction of $^8$He was used.\footnote{The measured spectrum and the rate fit both give results that are consistent with zero $^8$He, while rough predictions (XXX cite?) indicate that the $^8$He proportion should not exceed 20\%. Meanwhile, the predicted spectrum does not change very significantly when the fraction is varied from 0 to 20\%, in comparison to the other sources of uncertainty (neutron and alpha quenching). Hence, 5.5\%, an ``inherited'' feature of this analysis, is as good a guess as any.} \autoref{sec:fitToyBackgrounds} discusses the uncertainty assigned to the \LiHe\ spectrum.

XXX show plot overlaying measured and predicted spectra.

\section{Cosmogenic fast neutrons}
\label{sec:bkgFastn}

XXX ref doc-10948. Fig. 25 from long paper.

As cosmic muons travel through the rock and other materials surrounding the ADs, they can eject fast neutrons from the medium. If a fast neutron of the appropriate energy thermalizes and stops inside the AD, it will produce an IBD-like coincidence pair, in which the prompt signal consists largely of scintillation from scattered protons, and the delayed signal results from nGd capture. This process leads to a significant correlated background, amounting to some 20-30\% of that produced by cosmogenic isotopes. Two methods have been developed for estimating this background, the so-called \emph{extrapolation} and \emph{scaling} methods.

In the extrapolation method, the prompt energy cut of the IBD selection is extended past 12 MeV to 100~MeV or beyond, where true IBDs are completely absent and the (largely flat) spectrum consists almost entirely of fast neutrons. This spectrum is then extrapolated below 12~MeV to estimate the fast neutron component of the IBD sample, using a fit to a well-motivated model of the fast-n spectrum.

In the scaling method, a search is performed for ``muon-tagged'' IBD-like events in the immediate aftermath of ``peripheral'' muons that only trigger the outer water pool and/or RPC. As with the extrapolation method, the prompt energy cut is significantly extended. In the region above 12 MeV, where true IBDs are absent, the muon-tagging efficiency can be determined from the ratio of muon-tagged to untagged events. Then, below 12 MeV, the tagged spectrum (which contains very few true IBDs due to the short post-muon time window searched) is rescaled according to the tagging efficiency, yielding an estimate of the fast neutron spectrum within in the sub-12~MeV region.

The two methods are consistent to within 1--3\% (an order of magnitude smaller than the estimated uncertainty of each method), providing a high level of confidence in the estimation. In the following sections, we describe these methods, and their results (obtained by Hu, Ji, and Treskov, whom we abbreviate as HJT), in further detail.

\subsection{Event selection}
\label{sec:fastn_sel}

Two event samples are used in the fast neutron analysis. The first, which we shall refer to as the ``untagged'' sample is obtained using the standard IBD selection, as described in \autoref{chap:selection}, with the modification that the upper limit on prompt energy is extended to 300~MeV instead of the usual 12~MeV. In this sample, the prompt spectrum below 12~MeV is essentially the same as the one used in the oscillation fit (i.e., dominated by true IBDs), whereas the high-energy region almost exclusively contains fast neutrons.

The other, ``tagged,'' sample contains IBD-like events that occur right after a muon that triggers \emph{only} the outer water pool. When a muon only strikes the OWS, as opposed to the IWS or AD, most of the muon-generated debris is unable to penetrate into the GdLS, but fast neutrons are an exception. This tagging therefore provides a highly pure sample of fast neutrons for analysis.

The tagged sample is obtained by extending the upper cut to 300~MeV (as in the untagged sample) while disabling the standard muon veto. An additional requirement is that the prompt signal be timestamped within (-300, 600)~ns of an OWS trigger (defined by NHit > 15, in this case). Furthermore, the delayed signal must occur at least 15 $\mu$s after the muon (to eliminate Michel electrons), and there must be no AD or IWS muons within 600 $\mu$s of the OWS muon. This selection results in fairly low statistics (amounting to a few hundred events in the near halls), but the size of the sample is still sufficient to provide strong constraints on the fast-neutron background.

\subsection{Scaling method}
\label{sec:fastn_scaling}

In the scaling method, we assume that the shape of the tagged spectrum is an accurate representation of the shape of the fast-neutron background. In other words, we assume that, for any given fast neutron, the probability of an associated OWS trigger is independent of the neutron's energy. Previous studies within the collaboration have supported the validity of this assumption.

\def\emax{\ensuremath{E_\mathrm{max}}} \def\ntag{\ensuremath{N_\mathrm{tag}}}
\def\nuntag{\ensuremath{N_\mathrm{untag}}}

We define the scaling factor $F$ as \[ F(\emax) = \frac{\nuntag[12, \emax]}{\ntag[12, \emax]}, \] i.e., the ratio of the integral of the two samples between 12~MeV and \emax. Essentially, $F$ represents the efficiency of the OWS-tagging procedure. The dependence on \emax\ reflects the arbitrary choice of the upper energy limit, which contributes to the uncertainty on the result (primarily due to stastical fluctuations in the small sample of tagged neutrons). HJT quantify this uncertainty by comparing the results for \emax\ of 80, 100, 120, and 150~MeV. In addition, for a \emph{fixed} \emax, there is a purely statistical uncertainty,
\[ \sigma_F(\emax) = F \cdot \sqrt{\nuntag^{-1}[12, \emax] + \ntag^{-1}[12,
    \emax]},
\]
which also contributes to the final uncertainty.

\def\nfn{\ensuremath{N_\mathrm{fid}}} \def\rfn{\ensuremath{R_\mathrm{FN}}}

In the untagged spectrum, the number of fast neutrons within the fiducial energy range of [0.7, 12]~MeV is determined simply as
\[ \nfn = F \cdot \ntag[0.7, 12]. \] Its statistical uncertainty, in turn, is
\begin{equation}
  \label{eq:fastn_scal_unc}
  \sigma_\mathrm{FN} = \sqrt{\ntag^2[0.7, 12]
    \cdot \sigma_F^2 + F^2 \cdot \ntag[0.7, 12]}.
\end{equation}
Finally, the normalized daily fast neutron rate is
\begin{equation}
  \label{eq:fastn_rate}
  \rfn = \frac{\nfn}{T_\mathrm{DAQ} \cdot \epsilon_\mu \cdot \epsilon_m},
\end{equation}
where $T_\mathrm{DAQ}$, $\epsilon_\mu$, and $\epsilon_m$ are the DAQ livetime, muon veto efficiency, and multiplicy cut efficiency for the untagged sample. This value is then combined with the extrapolation result, described next, and a total uncertainty is assigned to the combination.

\subsection{Extrapolation method}
\label{sec:fastn_extrap}

Previous simulation studies have found that the fast neutron spectrum can be accurately described by the PDF
\begin{equation}
  \label{eq:bkgFastnShape}
  f(E) = A \cdot \left( \frac{E}{E_0} \right)^{-a-E/E_0},  
\end{equation}
where $E_0$ and $a$ are fitted shape parameters, and $A(E_0, a)$ normalizes the PDF. When we float an additional normalization factor $N$ and perform a fit to some portion of the fast neutron spectrum, the best-fit $N$ represents the number of events ``under the curve'' from 0 to $\infty$~MeV.

In turn, if we have a (hypothetical) pure fast-neutron spectrum containing $N_\mathrm{fid}$ events in the fiducial region of [0.7, 12]~MeV, then the full spectrum ([0, $\infty$]~MeV) should contain an event count equal to
\[ N_\mathrm{tot} = \frac{N_\mathrm{fid}}{\int_{0.7}^{12} f(E; E_0, a)\,dE }. \] This $N_\mathrm{tot}$ is, by definition, the same as the $N$ introduced in the previous paragraph. Therefore, when we fit a portion of the fast neutron spectrum to the form
\begin{equation}
  \label{eq:fastn_extrap_form}
  S(E) = \frac{N_\mathrm{fid} \left( \frac{E}{E_0} \right)^{-a-E/E_0}}
  {\int_{0.7}^{12} \left( \frac{E}{E_0} \right)^{-a-E/E_0} },
\end{equation}
(in which $A$ cancels out) the best fit $N_\mathrm{fid}$ indicates the number of events in the fiducial region. The key to the extrapolation method is that this fit is performed outside the fidicial region, where the fast-n sample is uncontaminated.

Prior to carrying out the extrapolation procedure, HJT verified the validity and robustness of \eqref{eq:fastn_extrap_form} by fitting it to their WP-tagged samples. They used four fitting ranges, all starting at 0.7~MeV, and ending at 80, 100, 120, and 150~MeV. In all cases, they obtained satisfactory goodness-of-fit and consistent values of $E_0$. The disabling of the $a$ parameter was found to introduce negligible differences.

Finally, the fit was performed on the untagged sample, using the same four upper limits as before, but with the lower limit set to 12~MeV. The spread between the resulting four values was incorporated into the total uncertainty, as described in the next section. As with the scaling method, each value of $\nfn$ was converted to a livetime- and efficiency-normalized daily rate according to \eqref{eq:fastn_rate}.

\subsection{Final result and total uncertainty}
\label{sec:fastn_comb}

In total, Hu, Ji, and Treskov each obtained a total of eight estimates of \rfn\ for each hall, derived from four scaling ranges in the scaling method, and four fitting ranges in the extrapolation method. We use Hu's results, as her selection cuts were essentially identical to ours. She arbitrarily chose the 12-100~MeV scaling method as the source her nominal fast neutron rates, and it is those numbers that we employ in our own analysis.

Six uncertainties were added in quadrature to obtain the total. The first is the statistical uncertainty on the scaling factor $F$, described by \eqref{eq:fastn_scal_unc}. The second is the uncertainty from the choice of scaling range, which was determined from the difference between the highest and lowest fast-n rates across the four ranges used. The third is the analogue for the choice of fitting range. The fourth is the statistical uncertainty in the fitted value of $N_\mathrm{fid}$. The fifth is the systematic uncertainty resulting from the dependence of fit results on the choice of binning, which was determined by varying the bin widths and repeating the fits. Finally, the sixth component was obtained from the difference in results between the scaling and extrapolation methods. These uncertainties are summarized in Table~XXX, and the final results given in Table~XXX.

\newcommand\AmC{$^{241}$Am-$^{13}$C}

\section{AmC source}
\label{sec:bkgAmC}

To study the response of the detectors to neutrons, each AD was initially configured with a low-intensity ($\sim$0.7~Hz) \AmC\ neutron source in each of the three automated calibration units (ACUs) housed on the detector's lid. The \AmC\ sources are used, for instance, in determining the ratio of visible energies between $^{60}$Co and n-Gd events; this ratio is a necessary input in the calibration of the AdScaled reconstruction. In contrast to more traditional neutron sources such as $^{252}$Cf and $^{241}$Am-$^{9}$Be, the \AmC\ source was designed to avoid multi-neutron and gamma-neutron cascades (a potential correlated background). Furthermore, when not in use, the location and shielding of the sources ensures that neutrons do not infiltrate the GdLS region, protecting against correlated backgrounds from proton recoils followed by neutron capture.

In spite of these precautions, a rare mechanism can still produce correlated backgrounds: A neutron may scatter inelastically in the stainless steel against Fe, Cr, Mn, or Ni, producing prompt gammas (generally totalling less than 3 visible MeV), before thermalizing and being captured either by one of these four elements or by Gd in the GdLS overflow tank (producing a signal between 6 and 12 visible MeV). Although many of these gammas do not reach the scintillator, some do, and when this happens for both the prompt and delayed gammas, the resulting pair can sometimes pass the IBD selection criteria.

The first experimental suggestion of this background came from an observed excess of neutron-like (i.e. 6--12 MeV) \emph{uncorrelated} events in the top half of the detector. The expected neutron-like events (primarily fast neutrons and decays of cosmogenic $^{12}$B) are predicted to display vertical symmetry (as was explicitly confirmed for a high-purity sample of $^{12}$B candidates), so the asymmetric nature of this excess implicated the \AmC\ source as the origin. Subsequent MC simulations showed that these uncorrelated events (from neutron capture in the SS or Gd overflow tank) are associated with a correlated background. The evaluation of this so-called AmC background is detailed in \cite{Gu_2016}; here we briefly summarize the procedure and its results.

At first glance, the AmC background could be measured simply by removing the AmC sources from one detector and comparing the IBD candidate rate to an adjacent detector. However, given the low rate of this process (0.2--0.3 events/day/AD when all three AmC sources are installed) and the substantial ``background'' from IBDs, it would take an impractical amount of time to accumulate sufficient statistics, and the AdScaled calibration would be impaired in the meantime. On the other hand, it \emph{is} possible to directly measure the rate ($\sim$230/day, initially) of \emph{uncorrelated} events from neutron capture in the stainless steel. Furthermore, MC simulations can be used to relate the rates of uncorrelated and correlated events. Finally, a high-activity AmC source (HAS) can be used to benchmark the MC. These three insights together enable a relatively precise (i.e. to within 45\%) determination of the AmC background.

The following is the fundamental relationship used in the AmC background estimation:
\begin{equation*}
  R_{\mathrm{corr}} = R_{\mathrm{uncorr}} \times \xi = R_{\mathrm{uncorr}} \times \int_{0.7}^{\SI{12}{MeV}} f(E)\,dE.
\end{equation*}
Here, $R_{\mathrm{uncorr}}$ is the rate of uncorrelated neutron-like events produced by the AmC source, as measured directly from ordinary data. Meanwhile, $\xi$ is the ratio of correlated to uncorrelated events, as determined from a combination of simulations and HAS data. $f(E)$ is simply the differential value of $\xi$ as a function of prompt energy; i.e. $f(E)\,dE$ is the number of correlated events with $E_{\mathrm{p}}$ in [$E$, $E + dE$], per uncorrelated neutron-like event.

$R_{\mathrm{uncorr}}$ was measured trivially, simply by taking the difference in the number of neutron-like events between the top and bottom halves of each AD. All of the complexity lay in the determination of $\xi$. In principle, $\xi$ could be extracted directly from a MC simulation. Although the Daya Bay MC contains a detailed and accurate modeling of the detector materials and geometry (including the ACUs and the AmC sources themselves), any inaccuracies in the simulated physics could bias the obtained value of $\xi$. Accordingly, the HAS was designed and deployed in order to validate the simulations.

Compared to the AmC calibration source, with a rate of $\sim$0.7~Hz, the HAS was much more intense, producing $\sim$59 neutrons/s. Furthermore, the enclosure of the HAS consisted of a nearly solid cylinder of stainless steel, in order to maximize the rate of neutron captures and inelastic scatters. The HAS was placed on the lid of EH3-AD1 (AD4) in the summer of 2012, and data was collected for ten days. The number of uncorrelated HAS-induced neutron-like events was determined by subtracting the neutron-like samples between AD4 and the adjacent AD5, which observed $\sim$50,000 and $\sim$4,000 events, respectively.

Meanwhile, the number of \emph{correlated} AmC events was measured by taking the spectrum of IBD candidates in AD4, subtracting the accidental background, and then subtracting the (background-subtracted) IBD sample measured by AD5.\footnote{This procedure doesn't account for other correlated backgrounds in AD4, such as $^9$Li and fast neutrons, but their rates of $< 0.2$/d are insignificant compared to the 63 correlated events per day produced by the HAS.} Relating the uncorrelated and correlated rates gave a value of $\xi$, \emph{for the HAS}, of $(1.5\pm0.3)\times10^{-3}$. The Geant4 MC, on the other hand, returned a $\xi$ of $(1.2\pm0.1)\times10^{-3}$ for the HAS. The 25\% difference versus data was then assigned as an uncertainty of the MC. With the addition of the 20\% statistical uncertainty on the data, a total uncertainty of 30\% was assigned to $\xi$.

Compared to the HAS, the ordinary, i.e. low-intensity, AmC source (LAS) is expected to have a lower $\xi$, since it lies farther from the AD and has a lower density of surrounding stainless steel. For the LAS, the MC predicted a $\xi$ of $0.9\times10^{-3}$. Based on the MC/data comparison for the HAS, this value was scaled up by 25\% to $1.125\times10^{-3}$, with an uncertainty of 30\%. 

In addition to the rates, the prompt spectrum of the AmC background also required determination. Fortunately, excellent agreement was found between the data and MC prompt spectra for the HAS. Furthermore, similar agreement was found between the MC HAS and MC LAS prompt spectra, in spite of the differences in geometry and material between the HAS and the LAS. As such, any one of these spectra could have been chosen as a reference. The choice was made to use the measured HAS spectrum, which was fit to an exponential function,
\begin{equation*}
  f(E) = p_0 \times e^{-E/p_1}.
\end{equation*}
The fit gave $p_1 = \SI{0.783}{MeV}$ with a 10\% statistical uncertainty, compared to \SI{0.794}{MeV} and \SI{0.830}{MeV} for the LAS and HAS MC samples, respectively. This 5\% spread, in combination with the 10\% statisical uncertainty, gave a conservative total uncertainty of 15\% on $p_1$ (essentially a shape uncertainty). Meanwhile, $p_0$ was fixed by the normalization condition $\int f(E)\,dE = \xi$, giving (for $\xi = 1.125\times10^{-3}$) $p_0 = \SI[parse-numbers = false]{3.606\times10^{-3}}{/MeV}$. Conservatively combining the 30\% uncertainty on $\xi$ with the 15\% uncertainty on $p_1$ gave a total uncertainty of 45\% on the AmC background. Given the identical design of the ADs, identical behavior was assumed with respect to the AmC background, and no attempt was made to calculate AD-specific quantities.

After the determination of $\xi$, the prompt spectrum (i.e. $p_1$), and the uncertainty, evaluation of each AD's AmC background then amounted to the simple task of measuring $R_{\mathrm{uncorr}}$ (after correcting for the muon veto efficiency) and multiplying it by $\xi$, resulting in the final values used in this analysis. It should be noted that the ACU-B and ACU-C AmC sources were removed from the EH3 ADs in 2012, during installation of EH2-AD2 and EH3-AD4.\footnote{In principle, the effective value of $\xi$ could vary between the three-sources and one-source scenarios, but this subtlety is not discussed in \cite{Gu_2016}. Presumably, any such effects fall within the 45\% uncertainty.} This significantly reduced the AmC background at the far site from 0.3\% to 0.1\% of the IBD rate. Furthermore, over the first two years of data, a 50\% decline was observed in the rate from each AmC source, in all three halls, likely due to leakage of scintillator into the source enclosures. This led to an ultimate background rate of only 0.05\% and 0.03\%, near and far (although the mean rate over the entire data sample is higher, due to the fact that earlier rates were higher.)

\section{$\CanO$}
\label{sec:bkgCanO}

A final and relatively minor ($\lesssim$ 0.01\%/0.07\% near/far) correlated background arises from $\alphN$ reactions initiated by natural radioactivity within the detector. In these reactions, an alpha from natural radioactivity is captured by a nucleus, which then emits a neutron. A prompt signal arises from a number of sources of energy deposition, including the kinetic energy of the alpha, gammas from nuclear deexcitation (including potentially those from inelastic scattering of the neutron), and proton recoils caused by the neutron. This prompt signal is then followed by capture of the neutron, mimicking the signature of an IBD.

Based on the chemical composition of the scintillator and the known cross sections of $\alphN$ reactions, it was determined that $\CanO$ is the only such reaction to occur in the ADs at any significant rate. Meanwhile, there are three natural decay chains that can lead to alpha activity in the AD: The so-called uranium, thorium, and actinium chains, which begin, respectively, with the long-lived isotopes $^{238}$U, $^{232}$Th, and $^{235}$U\footnote{In practice, the rate of the thorium chain is determined by the concentration of the shorter-lived $^{228}$Th ($t_{1/2}$ = 1.9~yr), and likewise, for the actinium chain by $^{227}$Ac ($t_{1/2}$ = 21.8~yr).}. Given that U, Th, and Ac all have similar chemical properties to Gd, a small amount of contamination is difficult to avoid during the Gd-doping process. In addition to these three decay chains, additional alpha activity comes from the decay of $^{210}$Po, a moderately stable ($t_{1/2}$ = 138~d) daughter of $^{222}$Rn (which itself comes from the uranium chain). $^{210}$Po was deposited on detector surfaces by $^{222}$Rn during detector construction, and is essentially the only significant alpha emitter outside the GdLS region.

Quantifying the $\CanO$ background consists of two parallel tasks. One task is to determine the level of alpha activity produced by the three decay chains and by $^{210}$Po. The other task is to determine, for the set of alphas produced by a given chain, the probability and prompt spectrum of $\CanO$ events. These two pieces of knowledge can then be combined to yield a predicted rate and spectrum for the $\CanO$ background.

The three chains all share a fortuitious property that enables a straightforward estimation of their rates. Namely, they each contain a rapid $\alpha$-$\alpha$ or $\beta$-$\alpha$ cascade whose time correlation and energy distribution allow for clean extraction from the data. For the uranium, thorium, and actinium chains, these cascades are, respectively, $^{214}$Bi $\to$ $^{214}$Po $\to$ $^{210}$Pb, $^{212}$Bi $\to$ $^{212}$Po $\to$ $^{208}$Pb, and $^{219}$Rn $\to$ $^{215}$Po $\to$ $^{211}$Pb, with Po half-lives of \SI{164.3}{\micro s}, \SI{0.3}{\micro s}, and \SI{1.781}{ms}.

To extract these events, time coincidence windows of [10, 400]~\us, [1, 3]~\us, and [1, 4]~ms were used, respectively. Accidentals (most significant for the actinium chain's Po cascade) could be subtracted via the usual procedure (XXX check that this was actually done, at least for actinium), and for the uranium chain's Po cascade, contamination with nH IBDs was not an issue given that the (quenched) delayed alpha energy for these events is around 1-1.5~MeV, significantly below the 2.2~MeV nH peak. For the thorium chain, the prompt spectrum had to be extrapolated below 0.5~MeV in order to determine the total rate; otherwise, there were no major complications. Under the assumption that each chain is in equilibrium\footnote{Up to $^{228}$Th and $^{227}$Ac for the thorium and actinium chains, rather than all the way up to $^{232}$Th and $^{235}$U, as noted previously.}, the rate of the polonium cascade gives the rate of the entire chain.

For the first two years of data, this procedure determined rates of 0.009, 0.16, and 0.2~Bq for U, Th, and Ac. Since the U chain is initiated by $^{238}$U in the AD, its rate is essentially constant, given the $^{238}$U half-life of 4.5~Gyr. On the other hand, the Th and Ac rates do decrease over time, since the parent half-lives (i.e. those of $^{228}$Th and $^{227}$Ac) are 1.9 and 21.8~yr. To determine the average Th and Ac rates for the 5+ year dataset used in this analysis, we account for this decrease and obtain XXX.

For $^{210}$Po, a single decay instead of a chain, time correlations could not be exploited. Instead, 5.3~MeV alphas produced by this isotope were, after quenching, visible as a peak around 0.5~MeV in the singles spectrum. Fitting this peak gave an average rate, for the first two years, of 0.7~Hz. Based on the $^{210}$Po half-life of 138~d, this was extrapolated to give an average rate in our dataset of XXX.

For a given decay chain (or $^{210}$Po), the set of emitted alphas is known. For each of these alphas, in turn, simulations can be used to determine the rate and prompt spectrum of $\CanO$ events. At each step in the simulation, the alpha loses some energy and travels some distance according to its $dE/dx$ profile in the LS. With some probability (i.e. cross section), during this step the alpha may be captured, producing one of the excited states of $^{17}$O. If this happens, the $^{17}$O will emit a neutron, whose energy depends on both the initial excited state of $^{17}$O and the final (excited or ground) state of $^{16}$O. The neutron produces prompt energy through proton recoils and, if it is sufficiently energetic, may scatter inelastically on $^{12}$C to produce a $\sim$5~MeV gamma. Additional prompt energy will come from deexcitation gammas if the $^{16}$O had been produced in an excited state. Repeating this procedure in a large Monte Carlo sample then gives the desired rate and prompt spectrum of $\CanO$ events for each alpha source.

Uncertainties in the $\CanO$ prediction arise from a number of sources. The uncertainty coming from the $\alphN$ cross section was estimated by repeating the MC procedure using two different cross section tables, JENDL and EXFOR. This suggested an uncertainty ranging from 6.6\% (for $^{210}$Po) up to 27.5\% (for $^{232}$Ac) \cite{Zhao_2014}. Additional uncertainty could come from the fundamentals of the MC simulation, i.e., the $dE/dx$ table and the numerical integration of discrete steps. This was evaluated by comparing the results of GEANT4 and SRIM, which differed at a negligible level of less than a percent. Finally, the assumption of decay chain equilibrium, and the efficiency of the cascade selection, both could introduce additional uncertainty. These are difficult to explicitly quantify, and given the relative unimportance of the $\CanO$ background, the decision was made to simply assign a conservative 50\% total uncertainty to the $\CanO$ estimation

XXX refer to CPC alpha-n paper.

\end{document}
