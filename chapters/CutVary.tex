\documentclass[../thesis.tex]{subfiles}

\begin{document}

\chapter{Variation of IBD selection cuts}
\label{chap:cutVary}

\section{Introduction}
\label{sec:cutVaryIntro}

The IBD selection described in \autoref{chap:selection} involves a number of numerical parameters, or \emph{cuts,} which must be specified. For the official IBD selection, these cuts were essentially chosen arbitrarily at an early stage in the experiment's history, although heuristic arguments were used to justify the claim that the chosen values would provide reasonable signal statistics and background levels. Some of the cuts differed between different analyses, which nonetheless produced consistent results, thus providing some indication of the robustness of the oscillation analysis with respect to variations in the cuts. Still, prior to this work, a systematic investigation of cut variations had never been undertaken, leaving open the question: Are Daya Bay's reported values of $\SinSq$ and $\Dmsqee$ sensitive to the precise values of the IBD selection cuts, and if so, to what extent? Naturally, any such sensitivity would need to be included as an additional source of systematic uncertainty. In this chapter, which constitutes a novel contribution to the high-level Daya Bay analysis, we demonstrate that Daya Bay's result is indeed insensitive to reasonable variations of the cuts.

Aside from the stability of the best-fit oscillation parameters as functions of the cuts, an additional consideration is the size of the uncertainty reported by the fitter. In principle, as the cuts are varied, there will be changes in the balance of efficiencies, raw statistics, and background rates, all of which can influence the size of the final error bars. It is therefore possible that the official IBD selection may be suboptimal in terms of minimizing the final uncertainty, and thus a secondary goal of this study is to determine whether an alternative ``optimal'' set of cuts can be found. If so, the final uncertainty will be improved, and if not, we will have shown a further aspect of the analysis's robustness.

In total, there are 12 cut parameters that enter the IBD selection:

\begin{itemize}
\item Minimum and maximum prompt energy
\item Minimum and maximum delayed energy
\item Minimum and maximum prompt-delayed time separation
\item Water pool muon charge threshold and veto time
\item AD muon charge threshold and veto time
\item Shower muon charge threshold and veto time
\end{itemize}

We do not separately consider the parameters of the decoupled multiplicity cut (DMC), as these are fixed by the prompt/delayed energy and time separation cuts. Among the 12 cuts listed above, most will, upon varying, have minimal impact on the analysis. The maximum prompt/delayed energy of 12~MeV is well above the endpoint of both spectra. Likewise, there are very few events at the time-separation limits of 1 and 200~\us. The shower muon veto window is three orders of magnitude longer than that of the WP and AD muon vetos; thus, as long as the latter two vetos can identify muons with $\sim$100\% efficiency while vetoing sufficiently long to remove prompt cosmogenic events, they are ``good enough''. This leaves four cuts which may have a significant effect upon the analysis: The shower muon threshold and window, the minimum delayed energy, and the minimum prompt energy. In subsequent sections, we discuss the reasons to expect these four cuts to be potentially impactful. Aside from these four cuts, we will also explore the effects of applying a vertex (i.e., position) cut, a nonstandard addition to the analysis which can provide a further demonstration of its overall robustness.

In order to carry out this study, significant modifications needed to be made to the LBNL oscillation analysis. The original LBNL IBD selection was intended to be run just once per data set, without any variation of the cut parameters, which were hardcoded and scattered throughout the code. As such, a new, general-purpose event processing framework was written from the ground up. The IBD selection was then implemented on top of this framework, with all cut parameters specified just once in an external text file, eliminating any need to recompile the code for each new set of cuts. As opposed to the previous IBD selection, which ran in a single pass, the new one takes a two-stage approach, with an initial, cut-independent pre-selection stage that performs a data reduction on the full processed Daya Bay data files, extracting only those events and variables of potential interest to the IBD selection. (Flashers and nonphysical triggers are removed at this stage, along with the vast majority of event variables, leaving only the reconstructed charge and energy, trigger time, and reconstructed vertex.) The second stage then applies a given set of IBD cuts to perform the selection. Computational efficiency was a primary consideration in the development of this code, in order to enable the reprocessing of data with many different cuts.

In addition to the IBD selection, the fitter also required a significant amount of work, although in this case a ground-up rewrite was not necessary. The only consideration here was computational efficiency, since each IBD selection must be followed by its own fit. Originally, the full fitting chain (including the toy Monte Carlo) took a couple of hours to run on NERSC's Cori cluster. After aggressively parallelizing the code, both by running independent steps in parallel processes, and by using OpenMP to harness multithreaded data parallelism within each process, the full chain was reduced to a runtime of around ten minutes. These changes, combined with the rewrite of the IBD selection, are what made this study possible.

Our overall strategy here is fairly straightforward: We simply run the IBD selection and fitter for a variety of different cut values, observing the behavior of both the best-fit points as well as the final reported uncertainties. As an additional sanity check, we generate toy MC samples assuming modified cut values, to verify that our understanding of the experiment, as encapsulated in the toy MC, does not predict any variation of the best fit as a function of the cuts. We do not expect any such variation in the best fit; however, it is possible that the toy MC will predict a variation in the size of the error bar, which can be compared to the results from fitting to data. Each cut will be thus studied individually, using both real data and the toy MC\@. Having drawn conclusions (regarding analysis robustness and, potentially, cut optimization) from these individual studies, we will then perform an ultimately study in which these cuts are randomly varied jointly and applied to our dataset, with the aim of providing final confirmation that the analysis is not biased by our particular choice of cut values.

Unfortunately, running the IBD selection with modified cuts requires more than simply specifying the new cuts; depending on the cut in question, there may also be the need to account for changes in efficiencies and background rates. In some cases, such as the accidentals rate, the multiplicity cut efficiency, and the veto efficiency, the correct value is automatically calculated by the IBD selection. However, other values, such as the correlated background rates, were externally calculated under the assumption of the nominal cut values, and so we must derive corrections for each and apply them during the process of converting the output of the IBD selection into the input for the fitter. We detail these calculations in the discussion of the corresponding cuts.

\section{Shower muon threshold and veto time}
\label{sec:cutVaryShowerMuon}

The shower muon veto parameters are important for two reasons: They significantly affect the overall veto efficiency (and thus the signal statistics), and they determine the rate of $^9$Li, which is a major source of background uncertainty, contributing more than 50\% (30\%) of the total near-site (far-site) background uncertainty. Altering these parameters can thus potentially affect both the statistical and systematic uncertainty of the oscillation fit. Furthermore, variation of these parameters can serve to demonstrate that the analysis gives consistent results under the ensuing variations in the event sample, and that the $^9$Li background subtraction is performed properly.

As described in \autoref{sec:bkgLi9LinReg}, our $^9$Li calculation properly captures the dependence of the rate upon the veto parameters. Likewise, the veto efficiency calculation simply sums up the unvetoed time windows, and is thus valid for any set of veto parameters. Both of these calculations are performed by the IBD selector. No additional steps need to be taken when reprocessing the data.

On the other hand, when we investigate the toy MC's predictions of the effects of varying the cut, we are not re-running the IBD selector, so we must explicitly adjust the veto efficiency and $^9$Li rate. Even though the toy MC produces is its own sample of ``data'', it does rely on an input, the so-called ``\texttt{Theta13}'' text file, which is produced by the IBD selector. This file lists, for each AD, the livetime, target mass, efficiencies (veto, multiplicity, delayed energy), and background rates and uncertainties. These values are then used by the toy MC to generate the predicted prompt spectra at each AD\@.

An additional input from the IBD selection is the singles spectrum (which gives the spectral shape of the accidental background). The AD muon veto window of O(1~ms) (to say nothing of the O(1~s) shower window) is more than long enough to remove all prompt, isolated, muon-induced triggers, so we expect no variation in the shape of the singles spectrum as we vary the shower veto.

The toy MC implementation of this study, then, requires taking the \texttt{Theta13} file produced by the nominal selection, and adjusting the veto efficiency, the $^9$Li rate, and its uncertainty. The veto efficiency correction is described in the following section. For the $^9$Li rate and uncertainty, we simply load the \texttt{Li9Calc} class from the IBD selector, and feed it the modified veto parameters. Once the \texttt{Theta13} file has been thus modified, the toy MC will generate toy spectra that properly account for the expected effects on the veto efficiency and $^9$Li rate.

\subsection{Veto efficiency calculation}%
\label{sec:cutVaryMuVetoEff}

In order to use the toy MC for predicting and validating the data-driven optimization of the shower muon veto, we must provide the overall muon veto efficiency as an input to the toy MC (and subsequently to the fitter). During the analysis of real data, the veto efficiency is computed as one of the outputs of the IBD selection. In principle, then, for the toy MC study we could simply run the IBD selection, with the desired veto parameters, in order to obtain the time-averaged efficiency for a given definition of the veto. However, this is computationally expensive, and more importantly, it would be ideal for the toy MC cross check to be as decoupled as possible from the IBD selection and its associated data. Thus, we seek an independent method of determining the muon veto efficiency.

The veto efficiency is, naturally, a function of the muon rate. In particular, it depends on the rates of the three types of muons: water pool, AD, and shower. The first step, then, is to measure these rates, while taking care to avoid double counting due to retriggers and multi-detector muons. For this purpose, a ``muon selector'' was written to extract all of the muon events in our dataset. This is the one place where actual data enters this process, but the code is independent of the IBD selection's second stage, where IBD cuts (including the veto) are normally applied. Once the muon sample has been obtained, the rates of the three classes can be calculated. The final step is to calculate the efficiency, which requires a proper treatment of the possible overlap of veto windows between closely spaced muons. In what follows we describe these steps in detail and demonstrate that the results agree well with the veto efficiency obtained from the IBD selection.

\subsubsection{Muon rate measurement}%
\label{sec:cutVaryMuRate}

The objective of the muon rate measurement is to obtain, for each AD, two results: First, the rate of \emph{water pool-only} muons (i.e., those muons which produced a ``WP muon'' trigger without an associated ``AD muon'' trigger), and second, the spectrum (in terms of charge or energy) of those muons that triggered the AD\@. Given that the AD/shower veto windows are longer than the WP window, any AD+WP muon should be regarded as a single AD muon. The measurement of the AD muon \emph{spectrum}, rather than the total rate, is important because it enables a breakdown into ``shower'' and ``non-shower'' AD muons, which carry different veto windows, and it makes this breakdown possible while varying the shower muon definition, without requiring a re-run of the muon selection.

The muon selector runs on the output of the first stage of the IBD selection, i.e., the pre-selection, which does not depend on the IBD selection cuts. In particular, the muon selector reads the contents of the \texttt{muons} tree, which simply contains basic information (time, detector, and charge or energy) for each WP trigger with more than 12 above-threshold PMTs, and each AD trigger with more than 3000 p.e.\ of charge. We collectively refer to these events of muon-like triggers.

If we were to naively count the number of events of each type in the \texttt{muons} tree, we would overestimate the muon rates, for two reasons: Muons that trigger more than one detector must still be only counted once, and muon-induced retriggers must be discarded. The strategy for handling these subtleties is similar to the one described in \autoref{sec:bkgLi9MuonSel} for the muon selection used in the $^9$Li rate study:

\begin{itemize}
\item No distinction is made between the inner and outer water pools.
\item If a WP muon occurs less than 15~\us\ after an AD muon, it is discarded in favor of the AD muon.
\item If a WP muon occurs less than 5~\us\ after another WP muon (which, in practice, will always be in the other pool), and the number of hit PMTs is greater than the previous one, this muon replaces the previous one.\footnote{This reflects the fact that the IBD selection's muon veto simply requires one pool, and not necessarily both, to be above-threshold.}
\item If a WP muon occurs between 5 and 15~\us\ after another WP muon, it is discarded as a retrigger.
\item If an AD muon occurs less than 15~\us\ after another muon in the same AD, it is discarded as a retrigger.
\item If an AD muon occurs less than 5~\us\ after a WP muon, the WP muon is discarded in favor of the AD muon.
\end{itemize}

Each final ``merged'' muon produced by this procedure is stored in a histogram binned according to the number of hits (for WP muons) or the charge (for AD muons). It should be noted that there is a separate histogram of WP muons for each AD, even though all ADs in a hall share the same water pools. This is because an AD muon will override a WP muon, but a given muon will not necessarily pass through every AD, so the WP muon will still be counted for those ADs which do not see an associated AD muon.

\begin{comment}
  XXX local slides from mid-late Oct for retrigger plots. See misc_ana/MuonVetoEff/condenser4retrig.

Are our efficiencies biased because we don't count for the'' `muon multiplicity efficiency' coming from being falsely ignored as a retrigger?
\end{comment}

Using the histograms of WP and AD muons, the rates of the three muon classes can be calculated: First, the number of WP(-only) muons is determined by integrating the WP histogram for \texttt{nHit}$> 13$, while the number of (non-shower) AD muons is the integral of the AD histogram from 3000 PE to the shower threshold, and the number of shower muons is the above-threshold integral. These counts are then divided by the total livetime to obtain the three rates.

\subsubsection{Efficiency from the rates}
\label{sec:cutVaryMuVetoEffFromRates}

The final step is to calculate the veto efficiency from the three rates. Although the three muon classes are simple independent Poisson processes, it is difficult to calculate the veto efficiency analytically, given the possibility of overlapping veto windows between and within the three classes. On the other hand, it is trivial to simulate the processes, and the simulated veto windows can then simply be added (after removing overlaps) to determine the total vetoed time and hence the efficiency. For the sake of validation, three different approaches were tested, all with consistent results:

\begin{itemize}
\item A ``shotgun'' toy MC that generates $N$ (rate $\times$ time) random muons (of each type) distributed in time according to the uniform distribution, before sorting and merging them
\item A ``sequential'' toy MC that uses the relative rates to determine the type of the ``next'' muon, and the exponential distribution to determine the time to it
\item A ``parallel'' toy MC in which the three processes independently generate random samples of the time-to-next (i.e.\ exponential) distributions, with the events then sorted and merged. This version generated output files in the same format as the pre-selector, so the actual second-stage IBD selection could be run on them, to verify that our window-adding calculation agrees with that in the IBD selector.
\end{itemize}

Fig.~XXX (doc-12269 p10) shows the daily efficiency of the nominal muon veto, both as reported by the IBD selection, and by performing the calculation above using the rates measured from data. As can be seen, there is a very small bias (on the order of 0.02\%) inherent in this approach. This is unsurprising, given that the IBD selection does not discard muon retriggers, which can therefore extend the veto window by O(10~\us). In any case, this bias is not large enough to meaningfully impact the toy MC study of the shower veto\footnote{And the whole purpose of the toy MC study is merely to show that we don't expect any variation of the fit results as we vary the veto parameters.}. Fig.~XXX (p11) compares the dependence of the efficiency upon the two shower veto parameters, as determined both by running the full IBD selection, and by using the above calculation on the muon rates measured by the nominal selection. Excellent agreement can be seen.

Having thus determined the expected veto efficiency for a modified shower veto, this value can be substituted into the \texttt{Theta13} file, along with the predicted $^9$Li rate. The toy MC can then be run as usual in order to generate the covariance and detector response matrices, etc., and then the Asimov toy spectra are fed to the fitter in order to extract the oscillation parameters. Meanwhile, for fits to data, the \texttt{Theta13} file is taken directly from the output of the IBD selection, and the fitter receives the prompt spectra from the data sample. In both cases, the toy samples (and hence the covariance matrices) are generated under the assumption of ``nominal'' oscillation parameters, namely $\SinSq = 0.084$ and $\Dmsqee = 0.00248$\footnote{As the covariance matrix has been shown to vary little under changes of the assumed oscillation parameters, we do not attempt to make the process more ``self-consistent'', such as by using an iterative procedure in which the covariance matrices are regenerated using the best-fit parameters, the fit is repeated, and so forth until convergence.}. In the following sections we discuss the fit results for both the toy and the real data samples.

\begin{comment}
Regarding results: Don't comment on ``structure'' until we've regenerated the 2D plots using the fix to SinglesCalc::calcSinglesHz. (Accidentals rate might have been biased, throwing off the fit.)
\end{comment}

\subsection{Fit results}%
\label{sec:cutVaryMuVetoDataResults}

Toy samples were produced and fit in a uniform 14x14 grid of shower veto parameters, with the charge threshold ranging from 1.8e5 to 4.4e5 photoelectrons, and the veto time ranging from 0.25 to 1.875 seconds. Fig.~XXX (doc p15 rhs) shows the results of the fits. The variation is on the order of 0.01\%, which is far below the scale of the analysis's uncertainty, and is likely attributable to rounding error of some kind. Thus, as expected, the toy MC predicts essentially no variation in the results of the oscillation analysis when the shower veto is varied.

Fig.~XXX shows the results of running the IBD selection and fitter on the P17B dataset, on the same grid of veto parameters as used in the toy MC study. In this case we see XXX.

Although the toy and data fits showed very disparate scales of variation in the best-fit oscillation parameters, the variation in the reported fit uncertainty was more consistent between the two. As expected, the toy MC predicts a small but non-negligible amount of variation in the uncertainty, due to the aforementioned effect of the shower veto on both the veto efficiency (i.e. signal statistics) and the $^9$Li rate. The fits to data indicate a slightly higher uncertainty overall, possibly due to statistical fluctuations in the data sample, but the same general pattern of increasing uncertainty toward the upper left of the grid. In any case, the key conclusion from both the toys and the data is that the nominal shower veto (using a threshold of 3e5 photoelectrons and a veto time of 0.4004~s) lies within a large, flat valley in the parameter space. We are free to vary the parameters within this region without consequence, but there is also nothing to be gained from it. The nominal ``LBNL'' shower veto, as well as the veto used by the alternative ``IHEP'' analysis (Analysis A in XXX long paper, vetoing for 1~s on muons of 2.5~GeV, or about 4e5 PE), can therefore both be considered ``optimal'' in the sense that they both achieve near-minimum uncertainty.

Meanwhile, the LBNL and IHEP vetos both lie in regions in which there is little variation in the best fit for small perturbations of the veto parameters. Curiously, however, there is a O(0.5\%) difference between the best fit values of $\SinSq$ for the LBNL and IHEP parameters (holding constant the remaining cuts). XXX We'd hope that both the LBNL and IHEP points give closer results. We'll see when we regenerate. Ideally, we see that the LBNL and IHEP points both lie within a large region of minimally varying best-fit $\SinSq$. Curious about the effects of using BCW binning. We should do both, so 14x14x2 = 392 sel/fits.

\section{Minimum delayed energy}
\label{sec:cutVaryMinDelayed}

In the reconstructed energy spectrum from nGd captures, there is a long and tall tail below the nominal cut of 6~MeV (Fig.~XXX). This tail is populated by events in which one or more gammas ``leak'' beyond the active volume of the AD, leaving some of the initial energy unreconstructed. Estimates (XXX?) suggest that some 10\% of nGd captures fall below the 6~MeV cut. Due to this potential of gaining O(10\%) in signal statistics, it is worthwhile to explore the variation of the 6~MeV cut. Here we study modified delayed cuts ranging from 4 to 7~MeV.

Of course, there is a tradeoff: With an increase in signal comes an increase in backgrounds. For those correlated backgrounds that involve an actual delayed nGd capture ($^9$Li, fast neutrons, and $\alpha$-$n$), the rate will increase according to the same scaling factor as the signal rate. The AmC background possesses a different delayed energy spectrum (domninated by neutron captures in the AD's stainless steel vessel), and this spectrum is unknown (XXX); due to the small size of this background, we simply ignore it in the present discussion.

Although these correlated backgrounds do increase, the effect on the uncertainty is outweighed by the increase in signal statistics. Letting $k$ denote this scaling factor (i.e.\ the ratio of the new delayed efficiency to the old one):
\begin{align}
  \label{eq:cutVaryDelayedNcorrSigma}
  & N_{\mathrm{corr}} \equiv N_{\mathrm{obs}} - N_{\mathrm{bkg}} \\
  \implies & \sigma(N'_{\mathrm{corr}}) = \sqrt{k} \sigma(N_{\mathrm{corr}}) \\
  \implies & \frac{\sigma(N'_{\mathrm{corr}})}{N'_{\mathrm{corr}}} = \frac{\sqrt{k} \sigma(N_{\mathrm{corr}})}{k N_{\mathrm{corr}}} = \frac{1}{\sqrt{k}} \frac{\sigma(N_{\mathrm{corr}})}{N_{\mathrm{corr}}},
\end{align}
that is, even though there is an increase in the absolute uncertainty on the background-subtracted rate, the \emph{relative} uncertainty actually decreases. Therefore, if these were the only backgrounds at Daya Bay, it would be advantageous to use a delayed cut that is as low as possible.

Unfortunately, the accidental background is not as well-behaved. As shown in Fig.~XXX, the spectrum of delayed-like singles rises much more steeply below 5 (XXX?) MeV compared to the neutron capture spectrum, implying that the accidental background increases faster (percentage-wise) than the IBD rate. When this effect is large enough, the overall uncertainty on the background-subtracted IBD rate no longer decreases (as illustrated in \autoref{eq:cutVaryDelayedNcorrSigma}); instead, it \emph{increases}. In principle, then, it is the accidental background that serves to penalize overly loose definitions of the delayed energy cut.\footnote{In saying this, we are ignoring the possibility of new, unaccounted-for \emph{correlated} backgrounds appearing when we loosen the delayed cut. As we discuss later, this is in fact a likely issue.}

A further downside of loosening the delayed cut is that the multiplicity cut (DMC) efficiency is reduced, as we are now more likely to reject an IBD candidate on the basis of an ``extra'' delayed-like event in the 200~\us\ post-delayed window. However, this effect is outweighed by the increase in the efficiency of the delayed cut, as shown (XXX) by the following argument. (Use Eq 6 of the AccAndDMC chapter, etc.).

In theory, the toy MC should be able to capture and quantify the \emph{known} effects of varying the delayed cut, that is, the effects on the delayed cut efficiency (and by extension the signal statistics), on the accidentals, and on the nominal set of correlated backgrounds. We expect the toy MC to show no variation in the best fit (again, as a confirmation of the self-consistency of the toy MC / fitter system), but we do expect the uncertainty on the oscillation parameters to vary, based on the balance of the competing effects described above.

As with the shower muon veto, we carry out the toy MC study by taking the \texttt{Theta13} file from the nominal (i.e. 6~MeV) IBD selection, and altering those quantities that vary with the delayed cut. However, the toy MC does not have any built-in knowledge of how the signal efficiency and background rates vary with the delayed cut. As such, we must externally provide this knowledge, which we obtain from measurements down to 4~MeV of the neutron capture spectrum and the delayed-like singles spectrum. These measurements are described in Sec.~XXX.

When we perform fits to actual data using a modified delayed cut, the correct accidentals rate is automatically calculated by the IBD selector, and of course there is no need to apply (as in the toy MC case) a scaling factor to the raw signal rate. However, the correlated background rates must still be corrected using the same formalism as that applied for the toy MC. Furthermore, an additional complication arises from the fact that the ADs differ among themselves in the shape of their neutron capture spectra (Fig.~XXX). As a result, when we modify the delayed cut, the efficiency of this cut will not necessarily vary in unison among the ADs, possibly altering the measured near-far ratio. There are multiple ways to deal with this issue, and we discuss them further in Sec.~XXX. First, however, we discuss our treatment of the backgrounds.

\subsection{Treatment of backgrounds}
\label{sec:cutVaryDelayedCutBkgTreatment}

\subsubsection{Accidentals background}

For fits to actual data, the accidentals background is calculated by the IBD selector as described in XXX AccAndDMC. This calculation is valid for any delayed energy cut, so no further correction is necessary.

For fits to toy spectra, we use information from a special IBD selection run on the full P17B dataset. This special selection is simply the nominal selection with a loosened delayed cut of 4~MeV. From the spectrum of delayed-like singles measured by this selection, we can calculate the accidentals rate for any delayed cut of 4~MeV or greater. We do this by manually instantiating the relevant class from the IBD selector. The calculated rates are then substituted into the \texttt{Theta13} file.

\subsubsection{Correlated backgrounds}

For both toy and data fits, we use the same procedure. Since $^9$Li, fast neutron, and $\alpha$-$n$ events all include a delayed neutron capture, we can employ the neutron capture spectrum $S_n(E)$obtained from IBDs, whose extraction we describe in Sec.~XXX. For a delayed cut of $q$~MeV, the backgrounds are then simply scaled according to the ratio
\[
  \frac{\int_q^{12} S_n(E)\,dE}{\int_6^12 S_n(E)\,dE}
\]

\section{Delayed cut efficiency}
\label{sec:cutVaryDelCutEff}

As we show later (XXX), naively varying the delayed cut will cause a shift in the best-fit oscillation parameters (particularly $\SinSq$). This shift comes from slight AD-to-AD differences in the shape of the IBD delayed spectrum. In what follows, we will refer to this spectrum as the ``neutron capture'' or ``nGd'' spectrum.\footnote{Although the bulk of delayed triggers are indeed nGd captures, there is in fact a small peak around 5~MeV (Fig.~XXX) from neutron captures on carbon.} Due to these shape discrepancies, varying the delayed cut will cause its efficiency to vary by different amounts between the ADs.

In the published Daya Bay analyses, we have always assumed that all 8 ADs have the same delayed cut efficiency, to which we assign a nominal (and irrelevant) value of 0.88 in the \texttt{Theta13} file. However, from the observed differences in the nGd shape, it is clear that this assumption is inaccurate. Obviously, the efficiency of the 6~MeV cut cannot be measured by a direct comparison of the IBD rates between ADs, since this wouldn't account for all of the other effects (e.g., oscillation!) which affect the measured rate. However, the delayed cut efficiency\footnote{More accurately, the total detection efficiency, divided by those efficiencies (veto and DMC) that we treat individually.} \emph{could}, in principle, be measured by a fitter with 8 pull terms for the efficiencies and an additional pull term for the ``reactor antineutrino anomaly'' (the last of which already is included in the pull-based fitters used in the collaboration). Of course, including these 8 additional pulls could lead to a shift in the best-fit $\SinSq$, reflecting the fact that a particularly unfortunate combination of delayed cut efficiencies could be mistaken for an oscillation signal.

Our fitter, however, does not support pull terms. Instead, we could take the measured IBD rates, correct them for baseline and oscillation effects, and then take the ratios to, say, AD1, giving the 8 \emph{relative} efficiencies, normalized to AD1. Again, though, such an approach would suffer from the ambiguity between the effects of oscillation and unequal efficiencies. Thus, we make no attempt to perform a corrected rate-based measurement of the 6~MeV cut's efficiency. Instead, our approach is to measure the shape of the neutron capture spectrum as best as possible (as described in Sec.~XXX), and use that as our sole source of information on the efficiency of the delayed cut. Since our study extends down to delayed cuts of 4~MeV, so does our measurement of the neutron capture spectrum. The soft end of this spectrum includes both events from the nGd tail as well as nC captures.

Once the neutron capture spectrum has been measured, there are three ways it can be used (or not) to apply a correction factor to the IBD rate. We name and define these approaches as follows:

\begin{itemize}
\item \texttt{flat}: No correction. As in the official analysis, we simply assume that all ADs have the same delayed cut efficiency, for all values of the cut.
\item \texttt{relative}: We integrate the spectrum from $E_{\mathrm{cut}}$ to 12~MeV, and divide by the integral from 6 to 12~MeV. Thus, when $E_{\mathrm{cut}} = 12$~MeV, we reproduce the official analysis.
\item \texttt{absolute}: We again integrate from $E_{\mathrm{cut}}$ to 12~MeV, but this time we divide by the total integral of the measured spectrum (i.e., from 4 to 12~MeV).
\end{itemize}

The \texttt{flat} method is obviously the simplest, and as we shall show (XXX), it produces the aforementioned shifts in $\SinSq$, demonstrating that the delayed cut efficiency varies among ADs. Meanwhile, the advantage of the \texttt{relative} correction is that it allows us to show that we can obtain constant results as we move the delayed cut away from 6~MeV, provided that we correct for differences in the neutron capture shape. This proves that we understand and can account for the implications of varying the cut. However, the \texttt{relative} method says nothing about the size of the bias caused by the assumption of equal efficiencies at 6~MeV. This is where the \texttt{absolute} method provides insight. From the difference in the best-fit values of $\SinSq$ at 6~MeV between the \texttt{relative} and \texttt{absolute} methods, we can infer the scale of the aforementioned bias. This measurement is still uncertain, since there is a degree of arbitrariness from the choice of the lower cutoff (in our case, 4~MeV), but as we show, there are not many neutron captures with such low energy, so that variations around 4~MeV can be expected to have minimal effect.

We will show the results of applying all three methods, ultimately demonstrating excellent stability in $\SinSq$ when the relative method is applied between 5 and 7~MeV.

\subsection{Measurement of neutron capture spectra}
\label{sec:cutVarDelCutSpecMeas}

Before we can integrate the neutron capture spectrum, we must, of course, measure it for each AD. It would be incorrect to use the raw delayed spectrum from the IBD candidates, since this includes contributions from backgrounds. Fortunately, all of the correlated backgrounds (except AmC, which we neglect due to its small rate) feature a neutron capture as the delayed event. On the other hand, accidental backgrounds will be drawn from the spectrum of delayed-like singles, which has a different shape from the neutron capture spectrum. Fortunately, both this spectrum and the accidentals rate can be measured quite accurately, so that we can obtain our final measurement simply by taking the delayed spectrum of IBD candidates, and subtracting the delayed-like singles spectrum scaled by the accidentals rate.

Although we did not diverge from this overall method, we implemented it in three different ways:

\begin{itemize}
\item \texttt{original}: We performed a ``reference'' IBD selection using a delayed cut of 4~MeV, extracting the neutron capture spectrum as described above. For any subsequent IBD selection with arbitrary cuts, this reference spectrum was used to obtain the (\texttt{relative} or \texttt{absolute}) efficiency.
\item \texttt{calc-then-add}: Within each IBD selection, we perform a ``parallel'' IBD selection using the same cuts, except with a 4~MeV delayed cut. The \emph{daily} spectrum, and then the efficiency (both \texttt{relative} and \texttt{absolute}) are obtained from the parallel selection. For the efficiency in the total sample, we use the weighted mean of the daily efficiencies.
\item \texttt{add-then-calc}: Again we use the ``parallel'' 4~MeV selection, but we do not calculate the efficiency on a daily basis. Rather, we add the daily neutron capture spectra in order to obtain the spectra for the full dataset, and then calculate the efficiency from these total spectra.
\end{itemize}

For all cut variations we study \emph{except for the application of a vertex cut}, these three methods generally give equivalent results. However, when we apply a vertex cut (as discussed in Sec.~XXX), there can be some variation in the shape of the neutron capture spectrum. The latter two methods were implemented so as to enable direct measurement of the efficiency for each cut, without recourse to a reference spectrum that may have the wrong shape. Although \texttt{calc-then-add} and \texttt{add-then-calc} are equivalent in principle, the former can suffer from division-by-zero in EH3 on days where there are no events in a given bin, so in practice we reject it in favor of \texttt{add-then-calc}.

In most of the results that follow, we show the outcome of using the \texttt{original} method, but the other methods give the same results. When we later discuss the application of vertex cuts, we will switch to the \texttt{add-then-calc} method (and will show the difficulties that result when attempting to use the \texttt{original} method).

\section{Minimum prompt energy}
\label{sec:cutVaryMinPrompt}

Unlike the delayed spectrum, the prompt spectrum of IBDs does not contain a long tail, so that varying the prompt cut is likely to have a smaller effect on the analysis, with minimal potential statistical gain. Although the minimum IBD energy deposition is 1.02~MeV (from positron annihilation), the finite detector resolution leads to the reconstruction of some events below this minimum energy. Since we can see that the prompt spectrum doesn't fall to zero at 0.7~MeV (Fig.~XXX), it is clear that events will be lost or gained upon variation of the cut. It is therefore worth verifying that the analysis is stable under such circumstances.

\emph{A priori,} there is no reason to assume that the prompt cut has the same efficiency (and $d\epsilon/dE_{\mathrm{cut}}$) across all ADs. Just as with the delayed cut, it is possible that we may need to apply corrections for the AD-dependent prompt efficiency. In that case, we could extract the prompt spectrum analogously to the case of the delayed spectrum, and then integrate it using either the \texttt{absolute} or \texttt{relative} methods.

Compared to the delayed spectrum, it may be possible to more accurately measure the absolute efficiency, given the lack of a long tail. Even though the rate of prompt-like singles rises very steeply around 0.7~MeV, the accidentals rate is bottlenecked by the rate of delayed-like singles, so the accidentals subtraction can still be performed accurately. However, the \emph{correlated} backgrounds each have their own externally supplied prompt spectra, which (as we describe XXX below) must be individually integrated (to scale the rate) and subtracted. Hence, the measured prompt spectrum could be skewed by any errors in the rates and spectra of the correlated backgrounds. Fortunately, the total rate of the correlated backgrounds is quite low, and in the case of the dominant one ($^9Li$), 0.7~MeV lies below its lower endpoint (XXX check and revise). We therefore don't expect the correlated backgrounds to pose a major difficulty in the prompt spectrum extraction.

The above discussion is irrelevant, however, because as we shall see, the \texttt{flat} method (in which we make no attempt to correct for the prompt efficiency) produces remarkably stable results. There is therefore no need to extract the prompt spectrum, so we will leave it at that.

When varying the prompt cut, then, the only adjustment we must make to the \texttt{Theta13} file is to correct the rates of the correlated backgrounds. Since we are in the possession of predicted prompt spectra $S_b(E)$ that extend down to 0~MeV for each correlated background $b$, we have all of the information needed. We simply take the nominal background rate (and uncertainty) and scale it according to the factor
\[
  \frac{\int_{E_{\mathrm{cut}}}^{12} S_b(E)}{\int_{0.7}^{12} S_b(E)}.
\]
The fitter, when subtracting the background, then takes $S_b(E)$, cuts it off at $E_{cut}$, normalizes it to the specified rate, and subtracts it from the raw prompt spectrum. This adjustment of the correlated backgrounds is applied for both the cases of fitting data and toy samples. There are no further subtleties related to the variation of the prompt cut.

As shown in Fig.~XXX, the toy MC predicts no shift in the best-fit oscillation parameters, as expected. Furthermore, there is no significant change in the total uncertainty, indicating that there is no obvious ``optimum'' prompt cut to use.

Meanwhile, Fig.~XXX shows the results of fitting actual data. Happily, there is no significant change in the best-fit values of $\SinSq$ and $\Dmsqee$ when we vary the prompt cut. Likewise, the uncertainty remains stable. Thus, with minimal effort, we have demonstrated that our analysis is stable against changes in the prompt cut, and there is nothing suboptimal about the nominal cut of 0.7~MeV.

\section{Vertex cut}
\label{sec:cutVaryVertexCut}

\end{document}
